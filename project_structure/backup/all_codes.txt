====== DATABASE SCHEMA BEGIN ======

-- TABLE: user_lang
CREATE TABLE user_lang (
            user_id INTEGER PRIMARY KEY,
            lang_code TEXT NOT NULL
        );


====== DATABASE SCHEMA END ======


---- FILE: config.py ----
# config.py

import os
from dotenv import load_dotenv

# Load environment variables from the .env file
load_dotenv()

# Determine the base directory of the project
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Telegram bot token retrieved from environment variables
TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

# Path to the log file, defaults to 'bot_log.log' if not specified
LOG_FILE = os.path.join(BASE_DIR, os.getenv("LOG_FILE", "bot_log.log"))

# Directory to store audio files, defaults to 'audio' if not specified
AUDIO_FOLDER = os.getenv("AUDIO_FOLDER", "audio")

# Administrator ID, retrieved from environment variables and converted to integer
# Defaults to 0 if not specified
ADMIN_ID = int(os.getenv("ADMIN_ID", "0"))


---- FILE: test.py ----
import os
import asyncio
import subprocess
import json
import requests
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command

# ------------ –í–∞—à–∏ —Ç–æ–∫–µ–Ω—ã ------------
TELEGRAM_TOKEN = '7288388195:AAF1J1u4eTplzAtawBTT8Q6-yJkWQL--KIQ'
WIT_AI_TOKEN = 'KDWYRKLXO7KA7WUEEG3LLH5ECUVGKQE4'
# -------------------------------------

bot = Bot(token=TELEGRAM_TOKEN)
dp = Dispatcher()

# ============ –®–∞–≥ 1. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è OGG -> WAV —á–µ—Ä–µ–∑ Docker + ffmpeg =============
async def convert_to_wav(ogg_path: str, wav_path: str):
    print(f"ogg_abs_path: {os.path.abspath(ogg_path)}")
    print(f"wav_abs_path: {os.path.abspath(wav_path)}")

    command = [
        "docker", "run", "--rm",
        "-v", f"{os.getcwd()}:/data",
        "jrottenberg/ffmpeg:4.4-alpine",
        "-y",
        "-i", f"/data/{os.path.basename(ogg_path)}",
        "-acodec", "pcm_s16le",
        "-ar", "16000",
        "-ac", "1",
        "-f", "wav",
        f"/data/{os.path.basename(wav_path)}"
    ]

    print("Running command:", " ".join(command))
    proc = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print("STDOUT:", proc.stdout.decode())
    print("STDERR:", proc.stderr.decode())

    if proc.returncode != 0:
        raise Exception(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ WAV: {proc.stderr.decode()}")

# ============ –®–∞–≥ 2. –†–∞–∑—Ä–µ–∑–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ WAV –Ω–∞ –∫—É—Å–∫–∏ –ø–æ 20 —Å–µ–∫—É–Ω–¥ ============
def split_wav(wav_path: str, chunk_seconds: int = 20) -> list[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç voice.wav –Ω–∞ —Ñ–∞–π–ª—ã chunk_000.wav, chunk_001.wav, ...
    –±–µ–∑ –ø–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (–ø—Ä–æ—Å—Ç–æ ¬´–Ω–∞—Ä–µ–∑–∫–∞¬ª).
    """

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ chunk_*.wav
    for fname in os.listdir("."):
        if fname.startswith("chunk_") and fname.endswith(".wav"):
            os.remove(fname)

    out_templ = "chunk_%03d.wav"

    # –ö–æ–º–∞–Ω–¥–∞ ffmpeg –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    command = [
        "docker", "run", "--rm",
        "-v", f"{os.getcwd()}:/data",
        "jrottenberg/ffmpeg:4.4-alpine",
        "-i", f"/data/{os.path.basename(wav_path)}",
        "-f", "segment",
        "-segment_time", str(chunk_seconds),
        "-c", "copy",
        f"/data/{out_templ}"
    ]
    print("Split command:", " ".join(command))
    proc = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print("SPLIT STDOUT:", proc.stdout.decode())
    print("SPLIT STDERR:", proc.stderr.decode())

    if proc.returncode != 0:
        raise Exception(f"–û—à–∏–±–∫–∞ –Ω–∞—Ä–µ–∑–∫–∏ WAV: {proc.stderr.decode()}")

    # –°–æ–±–∏—Ä–∞–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
    chunks = []
    for fname in sorted(os.listdir(".")):
        if fname.startswith("chunk_") and fname.endswith(".wav"):
            chunks.append(fname)
    return chunks

# ============ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ¬´—Å—ã—Ä–æ–≥–æ¬ª JSON-–æ—Ç–≤–µ—Ç–∞ ===========
def parse_json_objects(raw_text: str) -> list[dict]:
    """
    –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–æ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã { ... } –∏–∑ –æ—Ç–≤–µ—Ç–∞ Wit.ai.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (parsed JSON).
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –±–∏—Ç—ã–µ –∫—É—Å–∫–∏, –≤—ã–≤–æ–¥–∏—Ç –æ—à–∏–±–∫–∏ –≤ print.
    """
    depth = 0
    current = ""
    result = []

    for c in raw_text:
        if c == '{':
            depth += 1
            if depth == 1:
                current = '{'
            else:
                current += '{'
        elif c == '}':
            if depth > 0:
                depth -= 1
            current += '}'
            if depth == 0:
                # –∑–∞–∫–æ–Ω—á–∏–ª–∏ –æ–±—ä–µ–∫—Ç
                try:
                    data = json.loads(current)
                    result.append(data)
                except json.JSONDecodeError as e:
                    print("[parse_json_objects] JSON decode error:", e)
                current = ""
        else:
            if depth > 0:
                current += c

    return result

# ============ –®–∞–≥ 3a. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∫—É—Å–∫–∞ (–¥–æ 20 —Å–µ–∫—É–Ω–¥) ===============
def recognize_chunk(chunk_path: str) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ–º chunk_XXX.wav –Ω–∞ Wit.ai.
    –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≥–∏–ø–æ—Ç–µ–∑—ã.
    –ë–µ—Ä—ë–º —Ç—É, —á—Ç–æ –î–õ–ò–ù–ù–ï–ï –í–°–ï–• (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ —Å–∏–º–≤–æ–ª–æ–≤).
    """
    print(f" --- –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫—É—Å–æ–∫ {chunk_path} –≤ Wit.ai ---")
    with open(chunk_path, 'rb') as f:
        resp = requests.post(
            'https://api.wit.ai/speech?v=20220622',
            headers={
                'Authorization': f'Bearer {WIT_AI_TOKEN}',
                'Content-Type': 'audio/wav'
            },
            data=f
        )

    print(f"[{chunk_path}] Wit.ai status:", resp.status_code)
    if resp.status_code != 200:
        print(f"[{chunk_path}] –û—à–∏–±–∫–∞ Wit.ai: {resp.status_code}, {resp.text}")
        return ""

    raw_response = resp.text
    print(f"[{chunk_path}] RAW response:\n{raw_response}\n")

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã
    objects = parse_json_objects(raw_response)
    print(f"[{chunk_path}] –ö–æ–ª-–≤–æ JSON –æ–±—ä–µ–∫—Ç–æ–≤:", len(objects))

    # –ò—â–µ–º ¬´—Å–∞–º—É—é –¥–ª–∏–Ω–Ω—É—é¬ª —Å—Ç—Ä–æ–∫—É text
    best_text = ""
    for obj in objects:
        txt = obj.get("text", "")
        if len(txt) > len(best_text):
            best_text = txt

    best_text = best_text.strip()
    print(f"[{chunk_path}] –õ—É—á—à–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞: '{best_text}'\n")

    return best_text

# ============ –®–∞–≥ 3b. ¬´–£–º–Ω–∞—è¬ª —Å–∫–ª–µ–π–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—ã ========
def smart_concat(full_text: str, new_part: str) -> str:
    """
    –ù–∞ —Å—Ç—ã–∫–µ –∫—É—Å–∫–æ–≤ —É–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä.
    –ï—Å–ª–∏ –∫–æ–Ω–µ—Ü full_text —á–∞—Å—Ç–∏—á–Ω–æ –¥—É–±–ª–∏—Ä—É–µ—Ç –Ω–∞—á–∞–ª–æ new_part ‚Äî
    –ø–æ–¥—Ä–µ–∑–∞–µ–º new_part, —á—Ç–æ–±—ã –Ω–µ –ø–ª–æ–¥–∏—Ç—å "–ø—Ä–æ–±–ª–µ–º–∞? –ø—Ä–æ–±–ª–µ–º–∞?".
    """
    full_text = full_text.strip()
    new_part = new_part.strip()

    if not new_part:
        return full_text
    if not full_text:
        return new_part

    max_overlap = min(len(full_text), len(new_part))
    overlap_size = 0

    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (full_text[-i:] == new_part[:i])
    for i in range(max_overlap, 0, -1):
        if full_text.endswith(new_part[:i]):
            overlap_size = i
            break

    tail = new_part[overlap_size:].strip()
    if tail:
        if not full_text.endswith(" "):
            full_text += " "
        full_text += tail
    return full_text

# ============ –®–∞–≥ 3 (–æ–±—â–∏–π). –†–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤–µ—Å—å WAV, —Å–∫–ª–µ–∏–≤–∞–µ–º –∫—É—Å–∫–∏ =============
async def recognize_speech(big_wav_path: str) -> str:
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    chunks = split_wav(big_wav_path, chunk_seconds=20)
    print("–ù–∞–π–¥–µ–Ω–æ –∫—É—Å–∫–æ–≤:", len(chunks))

    final_text = ""
    for idx, ch in enumerate(chunks, start=1):
        print(f"–†–∞—Å–ø–æ–∑–Ω–∞—ë–º –∫—É—Å–æ–∫ {idx}/{len(chunks)}: {ch}")
        chunk_text = recognize_chunk(ch)
        # –°–∫–ª–µ–∏–≤–∞–µ–º
        final_text = smart_concat(final_text, chunk_text)

    if not final_text.strip():
        return "–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞."
    return final_text


# ===================== –•—ç–Ω–¥–ª–µ—Ä—ã —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç–∞ =====================
@dp.message(Command('start'))
async def cmd_start(message: types.Message):
    await message.answer("–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –¥–ª–∏–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ, –∞ —è –ø–æ–ø—ã—Ç–∞—é—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –µ–≥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω–æ.")


@dp.message(F.voice)
async def handle_voice(message: types.Message):
    ogg_path = "voice.ogg"
    wav_path = "voice.wav"
    try:
        file_info = await bot.get_file(message.voice.file_id)
        await bot.download_file(file_info.file_path, ogg_path)

        # 1) OGG -> WAV
        await convert_to_wav(ogg_path, wav_path)
        # 2) –†–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤—Å—ë (—Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º)
        text = await recognize_speech(wav_path)

        await message.reply(text)

    except Exception as e:
        await message.reply(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e).splitlines()[0]}")
    finally:
        # –û—á–∏—Å—Ç–∫–∞
        if os.path.exists(ogg_path):
            os.remove(ogg_path)
        if os.path.exists(wav_path):
            os.remove(wav_path)
        for fname in os.listdir("."):
            if fname.startswith("chunk_") and fname.endswith(".wav"):
                os.remove(fname)


async def main():
    await dp.start_polling(bot)

if __name__ == '__main__':
    asyncio.run(main())


---- FILE: bot.py ----
# ---- FILE: bot.py ----
import os
import asyncio
import chardet

from aiogram import Bot, Dispatcher, F
from aiogram.types import Message, FSInputFile
from aiogram.filters import Command
from aiogram.fsm.storage.memory import MemoryStorage
from loguru import logger

from config import TOKEN, AUDIO_FOLDER, LOG_FILE, ADMIN_ID

# i18n
from middlewares.i18n_middleware import I18nMiddleware

from utils.text_extraction import extract_text_from_url_static, extract_text_from_url_dynamic
from utils.text_to_speech import synthesize_text_to_audio_edge
from utils.document_parsers import parse_docx, parse_fb2, parse_epub
from utils.i18n import get_translator, get_user_lang, set_user_lang

from collections import deque
from pathlib import Path

# –ü–æ–¥–∫–ª—é—á–∞–µ–º middleware
from middlewares.rate_limit import RateLimitMiddleware
from middlewares.concurrency_limit import ConcurrencyLimitMiddleware

bot = Bot(token=TOKEN)
storage = MemoryStorage()
dp = Dispatcher(storage=storage)

dp.message.middleware(RateLimitMiddleware(rate_limit=5.0))
dp.message.middleware(ConcurrencyLimitMiddleware(max_concurrent_tasks=1))
dp.message.middleware(I18nMiddleware())

audio_path = Path(AUDIO_FOLDER)
audio_path.mkdir(parents=True, exist_ok=True)

logger.add(LOG_FILE, rotation="1 MB", retention="10 days", compression="zip")

MAX_MESSAGE_LENGTH = 4000


@dp.message(Command('start'))
async def cmd_start(message: Message, _):
    user_id = message.from_user.id
    lang_code = get_user_lang(user_id)
    translator = get_translator(lang_code)
    __ = translator.gettext  # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ

    await message.answer(_(
        "–ü—Ä–∏–≤–µ—Ç! –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ —Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∞–π–ª, –∞ —è –æ–∑–≤—É—á—É."
    ))


@dp.message(Command('lang'))
async def cmd_lang(message: Message, _):
    parts = message.text.split()
    if len(parts) < 2:
        await message.reply(_("Usage: /lang <en|ru|uk|zh>"))
        return

    new_lang = parts[1].strip().lower()
    if new_lang not in ['en', 'ru', 'uk', 'zh']:
        await message.reply(_("–î–æ—Å—Ç—É–ø–Ω—ã–µ —è–∑—ã–∫–∏: en, ru, uk, zh"))
        return

    set_user_lang(message.from_user.id, new_lang)

    t = get_translator(new_lang)
    await message.reply(t.gettext("–Ø–∑—ã–∫ –æ–±–Ω–æ–≤–ª—ë–Ω!"))


@dp.message(Command(commands=["s", "S", "—ã", "–´"]))
async def cmd_s(message: Message, _):
    if message.from_user.id != ADMIN_ID:
        logger.warning(f"–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â—ë–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {message.from_user.id}")
        await message.reply(_("Access denied."))
        return

    try:
        log_file = Path(LOG_FILE)
        if not log_file.exists():
            logger.error(f"–õ–æ–≥-—Ñ–∞–π–ª {LOG_FILE} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            await message.reply(_("Log file does not exist."))
            return

        with log_file.open('r', encoding='utf-8') as f:
            last_n_lines = deque(f, 15)
        last_lines = ''.join(last_n_lines)

        if not last_lines.strip():
            await message.reply(_("No log messages."))
            return

        messages_list = []
        current_message = ""
        for line in last_lines.splitlines(keepends=True):
            if len(current_message) + len(line) > MAX_MESSAGE_LENGTH:
                messages_list.append(current_message)
                current_message = line
            else:
                current_message += line
        if current_message:
            messages_list.append(current_message)

        for msg in messages_list:
            await message.reply(_("üìù Last log lines:\n") + msg, parse_mode='HTML')
            await asyncio.sleep(0.1)

    except Exception as e:
        logger.error(f"Failed to read log file: {e}")
        await message.reply(_("Failed to read log file: {error}").format(error=str(e)))


@dp.message(F.text.regexp(r'^https?://'))
async def handle_url(message: Message, _):
    url = message.text
    try:
        text_page = await extract_text_from_url_static(url)
        if len(text_page) < 200:
            text_page = await extract_text_from_url_dynamic(url)
        if not text_page.strip():
            await message.reply(_("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç."))
            return

        await synthesize_text_to_audio_edge(
            text_page,
            str(message.from_user.id),
            message,
            logger,
            _
        )

    except Exception as e:
        await message.reply(_("Failed to process URL: {error}").format(error=str(e)))


@dp.message(F.text)
async def handle_text(message: Message, _):
    text = message.text
    if not text.strip():
        await message.reply(_("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç."))
        return

    await synthesize_text_to_audio_edge(text, str(message.from_user.id), message, logger, _)


@dp.message(F.document)
async def handle_file(message: Message, _):
    if message.document.file_size > 20 * 1024 * 1024:
        await message.reply(_("–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (–º–∞–∫—Å–∏–º—É–º 20 –ú–ë)."))
        return

    file_extension = os.path.splitext(message.document.file_name)[1].lower()
    local_file_path = os.path.join(AUDIO_FOLDER, message.document.file_name)

    try:
        with open(local_file_path, "wb") as f:
            download_stream = await bot.download(message.document)
            f.write(download_stream.read())

        with open(local_file_path, 'rb') as f:
            raw_data = f.read()
        detected = chardet.detect(raw_data)
        encoding = detected['encoding']
        confidence = detected['confidence']
        logger.info(f"Detected encoding for {message.document.file_name}: {encoding} with confidence {confidence}")

        if file_extension == ".docx":
            text = parse_docx(local_file_path)
        elif file_extension == ".fb2":
            text = parse_fb2(local_file_path)
        elif file_extension == ".epub":
            text = parse_epub(local_file_path)
        else:
            if encoding is None:
                encoding = 'utf-8'
            with open(local_file_path, "r", encoding=encoding, errors='replace') as txt_f:
                text = txt_f.read()

        os.remove(local_file_path)

        if not text.strip():
            await message.reply(_("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞."))
            return

        await synthesize_text_to_audio_edge(text, str(message.from_user.id), message, logger, _)

    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {e}")
        await message.reply(_("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç: {error}").format(error=str(e)))
        if os.path.exists(local_file_path):
            os.remove(local_file_path)


---- FILE: main.py ----
# ---- FILE: main.py ----
import asyncio
from bot import dp, bot
from project_structure.paths import DATABASE_PATH
import sqlite3

def init_db():
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS user_lang (
            user_id INTEGER PRIMARY KEY,
            lang_code TEXT NOT NULL
        );
    """)
    conn.commit()
    conn.close()

async def main():
    init_db()
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())


---- FILE: middlewares/i18n_middleware.py ----
# ---- FILE: middlewares/i18n_middleware.py ----
import gettext
from aiogram.types import TelegramObject
from aiogram.dispatcher.middlewares.base import BaseMiddleware

import os

from utils.i18n import get_user_lang, get_translator

class I18nMiddleware(BaseMiddleware):
    async def __call__(self, handler, event: TelegramObject, data: dict):
        if hasattr(event, "from_user") and event.from_user:
            user_id = event.from_user.id
            lang_code = get_user_lang(user_id)
            t = get_translator(lang_code)
            data["_"] = t.gettext
        else:
            data["_"] = gettext.gettext

        return await handler(event, data)


---- FILE: middlewares/concurrency_limit.py ----
# middlewares/concurrency_limit.py
import asyncio
from aiogram import types
from aiogram.dispatcher.middlewares.base import BaseMiddleware
from typing import Callable, Dict, Any, Awaitable

class ConcurrencyLimitMiddleware(BaseMiddleware):
    """
    –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ "–¥–æ–ª–≥–∏–µ" –∑–∞–¥–∞—á–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–∞—Ä—Å–∏–Ω–≥+—Å–∏–Ω—Ç–µ–∑).
    - max_concurrent_tasks: –º–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
    - semaphores: dict[user_id -> asyncio.Semaphore]
    """
    def __init__(self, max_concurrent_tasks: int = 1):
        super().__init__()
        self.max_concurrent_tasks = max_concurrent_tasks
        self.semaphores = {}

    def get_semaphore_for_user(self, user_id: int) -> asyncio.Semaphore:
        """
        –°–æ–∑–¥–∞—ë—Ç (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        """
        if user_id not in self.semaphores:
            self.semaphores[user_id] = asyncio.Semaphore(self.max_concurrent_tasks)
        return self.semaphores[user_id]

    async def __call__(
        self,
        handler: Callable[[types.TelegramObject, Dict[str, Any]], Awaitable[Any]],
        event: types.TelegramObject,
        data: Dict[str, Any]
    ) -> Any:
        if isinstance(event, types.Message):
            user_id = event.from_user.id
            semaphore = self.get_semaphore_for_user(user_id)

            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–Ω—è—Ç—å "—Å–ª–æ—Ç"
            if semaphore.locked():
                # –£–∂–µ –≤—Å–µ —Å–ª–æ—Ç—ã –∑–∞–Ω—è—Ç—ã => –æ—Ç–∫–∞–∂–µ–º
                await event.answer(
                    "–£ –≤–∞—Å —É–∂–µ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–∞—è –∑–∞–¥–∞—á–∞. –î–æ–∂–¥–∏—Ç–µ—Å—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–µ–¥—ã–¥—É—â–µ–π."
                )
                return

            # otherwise, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            async with semaphore:
                # –ü–æ–∫–∞ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, —Å–ª–æ—Ç –∑–∞–Ω—è—Ç; –≤—ã—Ö–æ–¥–∏–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Äî —Å–ª–æ—Ç –æ—Å–≤–æ–±–æ–¥–∏–ª—Å—è
                return await handler(event, data)

        return await handler(event, data)


---- FILE: middlewares/rate_limit.py ----
# middlewares/rate_limit.py
import time
from aiogram import types
from aiogram.dispatcher.middlewares.base import BaseMiddleware
from typing import Callable, Dict, Any, Awaitable

class RateLimitMiddleware(BaseMiddleware):
    """
    –ü—Ä–æ—Å—Ç–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
    - –ù–µ –±–æ–ª–µ–µ 1 —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞ 'rate_limit' —Å–µ–∫—É–Ω–¥.
    """
    def __init__(self, rate_limit: float = 5.0):
        super().__init__()
        self.rate_limit = rate_limit
        # user_id -> timestamp –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø—Ä–∏–Ω—è—Ç–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        self.users_last_time = {}

    async def __call__(
        self,
        handler: Callable[[types.TelegramObject, Dict[str, Any]], Awaitable[Any]],
        event: types.TelegramObject,
        data: Dict[str, Any]
    ) -> Any:
        if isinstance(event, types.Message):
            user_id = event.from_user.id
            current_time = time.time()
            last_time = self.users_last_time.get(user_id, 0)

            if (current_time - last_time) < self.rate_limit:
                # –°–ª–∏—à–∫–æ–º —á–∞—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                await event.answer(
                    f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —á—É—Ç—å –ø–æ–∑–∂–µ (–ª–∏–º–∏—Ç: {self.rate_limit} c)."
                )
                return  # –ë–ª–æ–∫–∏—Ä—É–µ–º –¥–∞–ª—å–Ω–µ–π—à—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É

            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π —É—Å–ø–µ—à–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            self.users_last_time[user_id] = current_time

        return await handler(event, data)


---- FILE: utils/text_extraction.py ----
import asyncio
from newspaper import Article
from playwright.async_api import async_playwright

async def extract_text_from_url_static(url: str) -> str:
    """
    Attempt to extract text from a URL using the Newspaper library.
    This works best for static pages without heavy JavaScript.
    """
    article = Article(url)
    article.download()
    article.parse()
    return article.text.strip()

async def extract_text_from_url_dynamic(url: str) -> str:
    """
    Use Playwright to extract text from dynamically generated webpages.
    If static extraction fails or returns too little content,
    we fallback to dynamic extraction.
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto(url, timeout=60000)

            # Try a set of known selectors that often contain main content
            special_selectors = [".ReadTextContainerIn"]
            selectors = special_selectors + ["article", ".content", ".main-content"]
            article_text = None

            for selector in selectors:
                try:
                    if await page.locator(selector).count() > 0:
                        await page.wait_for_selector(selector, timeout=5000)
                        texts = await page.locator(selector).all_inner_texts()
                        article_text = "\n\n".join(texts).strip()
                        if article_text:
                            break
                except Exception:
                    continue

            # If no suitable selector is found, fallback to body text extraction
            if not article_text or article_text.strip() == "":
                article_text = await page.evaluate("document.body.innerText")

            return article_text.strip()
        except Exception:
            return ""
        finally:
            await browser.close()


---- FILE: utils/text_analysis.py ----
import re
from collections import Counter
import textstat
import nltk
from rake_nltk import Rake

# Download required NLTK data silently if not present
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

# Set of stopwords to be filtered out from the frequency count
STOP_WORDS = set(nltk.corpus.stopwords.words('russian')).union({
    "–≤", "–Ω–∞", "–∫", "–ø–æ", "—Å", "–¥–ª—è", "–∏", "–∏–ª–∏", "–æ—Ç", "–∏–∑", "–∑–∞", "–æ", "–æ–±",
    "—á—Ç–æ", "–Ω–µ", "—ç—Ç–æ", "–Ω–æ", "—Ç–∞–∫", "–∂–µ", "–∫–∞–∫", "–∫–æ–≥–¥–∞", "–µ—Å–ª–∏", "–≥–¥–µ", "–∫—Ç–æ",
    "–º–æ–∂–Ω–æ", "—Ç–æ–ª—å–∫–æ", "–±—É–¥–µ—Ç", "–ø—Ä–∏", "–∏–∑-–∑–∞", "–ø–æ—Ç–æ–º—É", "—Ç–æ–≥–¥–∞", "–≤–æ", "–±—ã",
    "—Ç–∞–º", "—Å—Ä–∞–∑—É", "–ø–æ–∫–∞", "–ª–∏", "—á—Ç–æ–±—ã", "—Å–µ–π—á–∞—Å", "–µ—â—ë", "–º–µ–∂–¥—É", "–¥–∞–∂–µ",
    "–º–æ–∂–µ—Ç", "–ø–æ—Å–ª–µ", "–ø–µ—Ä–µ–¥", "–ø—Ä–∏", "—Ç—É—Ç", "–¥–∞"
})

async def analyze_text(text: str) -> str:
    """
    Analyze the given text by:
    - Counting words and characters
    - Estimating reading time
    - Finding top-5 most common words (excluding stopwords)
    - Calculating reading difficulty via Flesch Reading Ease
    - Determining education level required
    - Extracting top key phrases via RAKE

    Returns a formatted summary string.
    """
    word_count = len(text.split())
    char_count = len(text)
    estimated_time_seconds = round(len(text) / 150)
    estimated_time_minutes = estimated_time_seconds // 60
    estimated_time_remainder_seconds = estimated_time_seconds % 60

    estimated_time_str = ""
    if estimated_time_minutes > 0:
        estimated_time_str += f"{estimated_time_minutes} min "
    if estimated_time_remainder_seconds > 0 or estimated_time_minutes == 0:
        estimated_time_str += f"{estimated_time_remainder_seconds} sec"

    # Find top words excluding stopwords
    words = re.findall(r'\b\w+\b', text.lower())
    filtered_words = [word for word in words if word not in STOP_WORDS]
    common_words = Counter(filtered_words).most_common(5)

    # Reading difficulty and grade level
    reading_ease = textstat.flesch_reading_ease(text)
    grade_level = textstat.text_standard(text, float_output=False)

    # Interpret reading ease
    if reading_ease > 80:
        reading_level = "Very easy to read"
    elif reading_ease > 60:
        reading_level = "Easy to read"
    elif reading_ease > 40:
        reading_level = "Moderately difficult"
    elif reading_ease > 20:
        reading_level = "Hard to read"
    else:
        reading_level = "Very hard to read"

    # Key phrases extraction using RAKE
    rake = Rake()
    rake.extract_keywords_from_text(text)
    key_phrases = rake.get_ranked_phrases()[:5]

    # Compile the summary
    summary_of_the_text = (
        f"üìù Your text contains {word_count} words and {char_count} characters.\n"
        f"‚è≥ Approximate narration time: {estimated_time_str}.\n\n"
        f"üìä <b>Text Analysis</b>:\n\n"
        f"- <b>Top-5 words</b>: {', '.join([f'{w} ({c})' for w, c in common_words])}\n"
        f"- <b>Reading level</b>: {reading_level} (Flesch: {reading_ease:.2f})\n"
        f"- <b>Suggested education level</b>: {grade_level}\n\n"
        f"- <b>Key phrases</b>: {', '.join(key_phrases)}\n"
    )

    return summary_of_the_text


---- FILE: utils/document_parsers.py ----
# utils/document_parsers.py
from xml.etree import ElementTree
from docx import Document
from ebooklib import epub, ITEM_DOCUMENT
from bs4 import BeautifulSoup
import warnings
from loguru import logger

# –ù–æ–≤–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å

def parse_docx(file_path: str) -> str:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ .docx-—Ñ–∞–π–ª–∞ –≤ —Ç–µ–∫—Å—Ç.
    ...
    """
    try:
        doc = Document(file_path)
        full_text = []
        for para in doc.paragraphs:
            paragraph_text = para.text.strip()
            if paragraph_text:
                full_text.append(paragraph_text)
        extracted_text = "\n".join(full_text)
        words_count = len(extracted_text.split())
        logger.info(f"[DOCX] –ò–∑–≤–ª–µ—á–µ–Ω–æ {words_count} —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ {file_path}")
        return extracted_text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ DOCX —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return ""

def parse_fb2(file_path: str) -> str:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ .fb2-—Ñ–∞–π–ª–∞ –≤ —Ç–µ–∫—Å—Ç.
    ...
    """
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        data_str = data.decode('utf-8', errors='replace')
        root = ElementTree.fromstring(data_str)
        ns = root.tag.split('}')[0].strip('{')
        namespaces = {'fb2': ns}

        sections = root.findall('.//fb2:section', namespaces=namespaces)
        text_chunks = []
        for sec in sections:
            paragraphs = sec.findall('.//fb2:p', namespaces=namespaces)
            sec_text = "\n".join([p.text.strip() for p in paragraphs if p.text])
            if sec_text.strip():
                text_chunks.append(sec_text)
        extracted_text = "\n\n".join(text_chunks).strip()
        words_count = len(extracted_text.split())
        logger.info(f"[FB2] –ò–∑–≤–ª–µ—á–µ–Ω–æ {words_count} —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ {file_path}")
        return extracted_text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ FB2 —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return ""

def parse_epub(file_path: str) -> str:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ .epub-—Ñ–∞–π–ª–∞ –≤ —Ç–µ–∫—Å—Ç.
    ...
    """
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            book = epub.read_epub(file_path)
        items = list(book.get_items_of_type(ITEM_DOCUMENT))
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ EPUB: {len(items)}")
        full_text = []

        for idx, item in enumerate(items, start=1):
            if item.get_type() == ITEM_DOCUMENT:
                content = item.get_content().decode('utf-8', errors='ignore')
                soup = BeautifulSoup(content, 'html.parser')
                for bad_tag in soup(["script", "style", "nav", "header", "footer", "meta", "link"]):
                    bad_tag.extract()
                paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                text_only = "\n".join(
                    para.get_text(separator=' ', strip=True) for para in paragraphs
                ).strip()

                logger.debug(f"[EPUB] –ß–∞—Å—Ç—å {idx}: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(text_only.split())} —Å–ª–æ–≤.")
                if text_only:
                    full_text.append(text_only)

        extracted_text = "\n".join(full_text)
        total_words = len(extracted_text.split())
        logger.info(f"[EPUB] –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å–ª–æ–≤: {total_words}")
        return extracted_text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ EPUB —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return ""

---- FILE: utils/text_to_speech.py ----
# ---- FILE: utils/text_to_speech.py ----
import time
import os
import edge_tts
from aiogram.types import Message, FSInputFile
from loguru import logger
from config import AUDIO_FOLDER
from .text_analysis import analyze_text
from langdetect import detect, DetectorFactory, LangDetectException

# –ß—Ç–æ–±—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ã–ª–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–º–∏
DetectorFactory.seed = 0

CHUNK_SIZE = 40000  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª-–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —á–∞—Å—Ç—å

VOICE_MAP = {
    "ru": "ru-RU-DmitryNeural",       # –†—É—Å—Å–∫–∏–π
    "en": "en-US-JennyNeural",        # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (–°–®–ê)
    "uk": "uk-UA-OstapNeural",        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–π
    "zh-cn": "zh-CN-XiaoxiaoNeural",  # –ö–∏—Ç–∞–π—Å–∫–∏–π (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π)
    # –î–æ–±–∞–≤—å—Ç–µ –¥—Ä—É–≥–∏–µ —è–∑—ã–∫–∏ –∏ –≥–æ–ª–æ—Å–∞ –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
}

def detect_language(text: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–¥ —è–∑—ã–∫–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, 'ru', 'en', 'uk', 'zh-cn'.
    –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 'unknown'.
    """
    try:
        lang = detect(text)
        lang = lang.lower()
        if lang.startswith("zh"):
            return "zh-cn"  # –∏–ª–∏ "zh-tw" –¥–ª—è —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–∏—Ç–∞–π—Å–∫–æ–≥–æ
        return lang
    except LangDetectException:
        return "unknown"

async def chunk_text(text: str, chunk_size: int = CHUNK_SIZE):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ chunk_size —Å–∏–º–≤–æ–ª–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size
    return chunks

async def synthesize_chunk(text: str, mp3_path: str,
                           voice: str = "ru-RU-DmitryNeural",
                           rate: str = "+50%"):
    """
    –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ—ë –≤ mp3_path.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å –∏ —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–µ—á–∏.
    """
    communicate = edge_tts.Communicate(text=text, voice=voice, rate=rate)
    await communicate.save(mp3_path)

async def synthesize_text_to_audio_edge(text: str, filename_prefix: str, message: Message, logger, _):
    """
    –°–∏–Ω—Ç–µ–∑–∏—Ä—É–µ—Ç *–±–æ–ª—å—à–æ–π* —Ç–µ–∫—Å—Ç –ø–æ —á–∞—Å—Ç—è–º –∏
    —Å—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–±–µ–∑ –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞).

    1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç (text_analysis) -> —à–ª—ë–º summary.
    2. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ –∏ –≤—ã–±–∏—Ä–∞–µ–º –≥–æ–ª–æ—Å.
    3. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏.
    4. –ö–∞–∂–¥—É—é —á–∞—Å—Ç—å —Å–∏–Ω—Ç–µ–∑–∏—Ä—É–µ–º, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º, —É–¥–∞–ª—è–µ–º.
    """
    if not text.strip():
        logger.warning(_("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å–∏–Ω—Ç–µ–∑–∞."))
        await message.reply(_("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–∑–≤—É—á–∏—Ç—å: –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç."))
        return

    if len(text) > 1000000:
        logger.info(_("–°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {len_text} —Å–∏–º–≤–æ–ª–æ–≤").format(len_text=len(text)))
        await message.reply(_("–¢–µ–∫—Å—Ç –¥–ª—è –æ–∑–≤—É—á–∫–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ù–∞–ø–∏—à–∏—Ç–µ @maksenro –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø–æ–≤—ã—Å–∏—Ç—å –ª–∏–º–∏—Ç—ã!"))
        return

    logger.info(_("@{username}, –¥–ª–∏–Ω–∞ - {len_text}: {preview}...").format(
        username=message.from_user.username,
        len_text=len(text),
        preview=text[:100]
    ))
    summary = await analyze_text(text)
    await message.reply(summary, parse_mode='HTML')

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞
    lang = detect_language(text)
    if lang == "unknown":
        logger.warning(_("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."))
        voice = "ru-RU-DmitryNeural"  # –ì–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        await message.reply(_("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–æ–ª–æ—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."))
    else:
        voice = VOICE_MAP.get(lang, "ru-RU-DmitryNeural")  # fallback –Ω–∞ —Ä—É—Å—Å–∫–∏–π
        logger.info(_("–û–ø—Ä–µ–¥–µ–ª—ë–Ω —è–∑—ã–∫: {lang}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≥–æ–ª–æ—Å: {voice}").format(lang=lang, voice=voice))

    base_filename = f"{text[:25]}_{filename_prefix}"
    parts = await chunk_text(text)
    total_parts = len(parts)

    for i, chunked_text in enumerate(parts, start=1):
        part_filename = f"{i}_of_{total_parts}_{base_filename}.mp3"
        mp3_path = os.path.join(AUDIO_FOLDER, part_filename)

        start_time = time.time()
        await synthesize_chunk(chunked_text, mp3_path, voice=voice)
        end_time = time.time()

        logger.debug(_("–ß–∞—Å—Ç—å {i}/{total_parts} —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–∞ –∑–∞ {seconds} —Å–µ–∫.").format(
            i=i,
            total_parts=total_parts,
            seconds=round(end_time - start_time, 2)
        ))

        audio_file = FSInputFile(mp3_path)
        await message.reply_audio(audio=audio_file)
        os.remove(mp3_path)


---- FILE: utils/i18n.py ----
# ---- FILE: utils/i18n.py ----
import gettext
import os
import sqlite3
from project_structure.paths import DATABASE_PATH

LOCALES_DIR = os.path.join(os.path.dirname(__file__), "locales")

def get_translator(lang_code: str):
    if lang_code not in ['en', 'ru', 'uk', 'zh']:
        lang_code = 'en'
    try:
        return gettext.translation(
            domain='messages',
            localedir=LOCALES_DIR,
            languages=[lang_code]
        )
    except FileNotFoundError:
        return gettext.translation(
            domain='messages',
            localedir=LOCALES_DIR,
            languages=['en']
        )

def get_user_lang(user_id: int) -> str:
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT lang_code FROM user_lang WHERE user_id = ?", (user_id,))
        row = cursor.fetchone()
        conn.close()
        if row:
            return row[0]
        else:
            return "ru"  # fallback
    except:
        return "ru"

def set_user_lang(user_id: int, lang_code: str):
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO user_lang (user_id, lang_code)
        VALUES (?, ?)
        ON CONFLICT(user_id) DO UPDATE SET lang_code=excluded.lang_code
    """, (user_id, lang_code))
    conn.commit()
    conn.close()


---- FILE: project_structure/paths.py ----
# ---- FILE: project_structure/paths.py ----
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent.parent
DATABASE_PATH = PROJECT_ROOT / 'MKvoiceDB.sqlite'

STRUCTURE_DIR = PROJECT_ROOT / 'project_structure' / 'backup'
STRUCTURE_DIR.mkdir(parents=True, exist_ok=True)

STRUCTURE_FILE = STRUCTURE_DIR / 'project_structure.txt'
IMAGES_STRUCTURE_FILE = STRUCTURE_DIR / 'images_structure.txt'
ARCHIVE_FILE = STRUCTURE_DIR / 'project_archive.zip'
DATABASE_SCHEMA_FILE = STRUCTURE_DIR / 'database_schema_telegram_messages.sql'
DB_DESCRIPTION_FILE = STRUCTURE_DIR / 'db_description.txt'

IGNORE_DIRS = {
    '.git', '.idea', '__pycache__', 'venv', 'env',
    '.venv', '.env', 'node_modules', 'backup'
}
IGNORE_FILES = {'.DS_Store', 'project_archive.zip'}
IMAGE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.svg', '.webp'}
IGNORE_EXT = {'.pyc', '.pyo', '.log', '.db', '.zip', '.tar', '.gz', '.bz2', '.7z', '.rar'}

LOG_DIR = PROJECT_ROOT / 'logs'
LOG_DIR.mkdir(exist_ok=True)


---- FILE: project_structure/project_structure_creator.py ----
import zipfile
import sqlite3
import os
from pathlib import Path

# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç: —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞–ø–∫–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è 'project_structure', –∞ –Ω–µ 'project_stucture'
from project_structure.paths import (
    PROJECT_ROOT,
    STRUCTURE_DIR,
    STRUCTURE_FILE,
    IMAGES_STRUCTURE_FILE,
    ARCHIVE_FILE,
    IGNORE_DIRS,
    IGNORE_FILES,
    IMAGE_EXTENSIONS,
    IGNORE_EXT,
    DATABASE_SCHEMA_FILE,
    DB_DESCRIPTION_FILE,
    DATABASE_PATH
)


def should_ignore(file_path: Path, relative_path: Path, ignore_dirs, ignore_files, ignore_ext) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å —Ñ–∞–π–ª/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–∏ –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–∏ (–∏ –≤ –¥—Ä—É–≥–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏—è—Ö).
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    for ignore_dir in ignore_dirs:
        ignore_dir_path = Path(ignore_dir)
        if ignore_dir_path in relative_path.parents:
            print(f"–ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è —Ñ–∞–π–ª/–ø–∞–ø–∫–∞ {relative_path} –∏–∑-–∑–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {ignore_dir}")
            return True

    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ —Ü–µ–ª–∏–∫–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, .DS_Store, project_archive.zip –∏ –ø—Ä.)
    if relative_path.name in ignore_files:
        print(f"–ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è —Ñ–∞–π–ª {relative_path} –∏–∑-–∑–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º–æ–≥–æ –∏–º–µ–Ω–∏")
        return True

    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
    if relative_path.suffix.lower() in ignore_ext:
        print(f"–ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è —Ñ–∞–π–ª {relative_path} –∏–∑-–∑–∞ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è")
        return True

    return False


def get_project_structure(root_dir: Path, output_file: Path, ignore_dirs):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–µ—Ä–µ–≤–æ —Ñ–∞–π–ª–æ–≤ –≤ output_file.
    """
    with output_file.open('w', encoding='utf-8') as f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            # –£–±–∏—Ä–∞–µ–º –∏–∑ dirnames —Ç–µ, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –≤ ignore_dirs (–ø–æ –∏–º–µ–Ω–∏),
            # —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ö–æ–¥–∏—Ç—å –≤–Ω—É—Ç—Ä—å. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ 'backup' –≤ ignore_dirs, –Ω–µ –∑–∞—Ö–æ–¥–∏—Ç—å –≤ 'backup'
            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            # –í—ã—á–∏—Å–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—Ç—Å—Ç—É–ø–æ–≤
            if relative_dir == Path('.'):
                level = 0
            else:
                level = len(relative_dir.parts)
            indent = '    ' * level
            f.write(f"{indent}{current_dir.name}/\n")

            subindent = '    ' * (level + 1)
            for name in filenames:
                f.write(f"{subindent}{name}\n")


def get_images_structure(root_dir: Path, image_extensions, output_file: Path, ignore_dirs):
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ö–æ–¥–∏—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞ –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–µ—Ä–µ–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ output_file.
    """
    with output_file.open('w', encoding='utf-8') as f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_files = [
                file for file in filenames
                if (current_dir / file).suffix.lower() in image_extensions
            ]
            if image_files:
                if relative_dir == Path('.'):
                    level = 0
                else:
                    level = len(relative_dir.parts)
                indent = '    ' * level
                f.write(f"{indent}{current_dir.name}/\n")
                subindent = '    ' * (level + 1)
                for name in image_files:
                    f.write(f"{subindent}{name}\n")


def export_database_schema(db_path: Path, output_file: Path):
    """
    –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—É—é —Å—Ö–µ–º—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –≤–∫–ª—é—á–∞—è —Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Å—ã, –≤—å—é—Ö–∏ –∏ —Ç—Ä–∏–≥–≥–µ—Ä—ã.
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        with output_file.open('w', encoding='utf-8') as f:
            # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã (—Ç–∞–±–ª–∏—Ü—ã, –∏–Ω–¥–µ–∫—Å—ã, –≤—å—é—Ö–∏, —Ç—Ä–∏–≥–≥–µ—Ä—ã),
            # –∏—Å–∫–ª—é—á–∞—è —Å–ª—É–∂–µ–±–Ω–æ–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä sqlite_sequence).
            cursor.execute("SELECT type, name, sql FROM sqlite_master WHERE sql IS NOT NULL;")
            objects = cursor.fetchall()

            for obj_type, obj_name, obj_sql in objects:
                f.write(f"-- {obj_type.upper()}: {obj_name}\n")
                f.write(f"{obj_sql};\n\n")

            print(f"–°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ '{output_file}'.")
        conn.close()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ —Å—Ö–µ–º—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")


def create_zip_archive(
    root_dir: Path,
    archive_path: Path,
    ignore_dirs,
    ignore_files,
    ignore_ext,
    extra_files_to_add=None
):
    """
    –°–æ–∑–¥–∞—ë—Ç zip-–∞—Ä—Ö–∏–≤ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –Ω–µ–Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã/–ø–∞–ø–∫–∏.
    –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ—Ç (–≤ —Ç–æ–º –∂–µ –ø–æ—Ç–æ–∫–µ) –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã (extra_files_to_add),
    —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.
    """
    added_rel_paths = set()

    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        # 1. –ü—Ä–æ–±–µ–≥–∞–µ–º—Å—è –ø–æ –≤—Å–µ–º—É –ø—Ä–æ–µ–∫—Ç—É –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å—ë, —á—Ç–æ –Ω–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            for filename in filenames:
                file_path = current_dir / filename
                rel_path = file_path.relative_to(root_dir)

                if should_ignore(file_path, rel_path, ignore_dirs, ignore_files, ignore_ext):
                    continue

                rel_path_str = str(rel_path).replace(os.sep, '/')
                if rel_path_str not in added_rel_paths:
                    zipf.write(file_path, arcname=rel_path_str)
                    added_rel_paths.add(rel_path_str)
                else:
                    print(f"–§–∞–π–ª '{rel_path}' —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∞—Ä—Ö–∏–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")

        # 2. –î–æ–±–∞–≤–ª—è–µ–º "—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ" —Ñ–∞–π–ª—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if extra_files_to_add:
            for special_file in extra_files_to_add:
                if not special_file.exists() or not special_file.is_file():
                    print(f"–§–∞–π–ª '{special_file}' –Ω–µ –Ω–∞–π–¥–µ–Ω –∏ –Ω–µ –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω –≤ –∞—Ä—Ö–∏–≤.")
                    continue

                # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
                try:
                    arcname = special_file.relative_to(root_dir)
                except ValueError:
                    # –ï—Å–ª–∏ —Ñ–∞–π–ª –≤–Ω–µ root_dir
                    arcname = special_file.name

                arcname_str = str(arcname).replace(os.sep, '/')
                if arcname_str not in added_rel_paths:
                    zipf.write(special_file, arcname=arcname_str)
                    added_rel_paths.add(arcname_str)
                    print(f"–î–æ–±–∞–≤–ª–µ–Ω —Ñ–∞–π–ª '{special_file}' –≤ –∞—Ä—Ö–∏–≤.")
                else:
                    print(f"–§–∞–π–ª '{special_file}' —É–∂–µ –µ—Å—Ç—å –≤ –∞—Ä—Ö–∏–≤–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
    print(f"–ê—Ä—Ö–∏–≤ '{archive_path}' —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω.")


def create_all_codes_file(
    root_dir: Path,
    output_file: Path,
    ignore_dirs,
    ignore_files,
    ignore_ext,
    database_schema_file: Path = None
):
    """
    –°–æ–∑–¥–∞—ë—Ç —Ñ–∞–π–ª all_codes.txt, –≤ –∫–æ—Ç–æ—Ä–æ–º:
    1. –°–Ω–∞—á–∞–ª–∞ (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏) –ø–∏—à–µ—Ç—Å—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ö–µ–º—ã –ë–î.
    2. –ó–∞—Ç–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö –Ω–µ–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã—Ö .py —Ñ–∞–π–ª–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
       ---- FILE: <–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å> ----
       <—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞>
    """
    with output_file.open('w', encoding='utf-8') as out:
        # 1. –î–æ–±–∞–≤–ª—è–µ–º —Å—Ö–µ–º—É –ë–î (–µ—Å–ª–∏ –µ—Å—Ç—å –∏ –µ—Å–ª–∏ –æ–Ω–∞ —Å–æ–∑–¥–∞–Ω–∞)
        if database_schema_file and database_schema_file.exists():
            db_schema_content = database_schema_file.read_text(encoding='utf-8', errors='replace')
            out.write("====== DATABASE SCHEMA BEGIN ======\n\n")
            out.write(db_schema_content)
            out.write("\n====== DATABASE SCHEMA END ======\n\n\n")
        else:
            out.write("-- –°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ –±—ã–ª–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ --\n\n")

        # 2. –û–±—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –ª–∏—à–Ω–∏–µ, –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ .py —Ñ–∞–π–ª—ã
        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            for filename in filenames:
                file_path = current_dir / filename
                rel_path = file_path.relative_to(root_dir)

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ç–µ–º –∂–µ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è
                if should_ignore(file_path, rel_path, ignore_dirs, ignore_files, ignore_ext):
                    continue

                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ .py —Ñ–∞–π–ª—ã
                if file_path.suffix.lower() != '.py':
                    continue

                out.write(f"---- FILE: {rel_path} ----\n")
                try:
                    content = file_path.read_text(encoding='utf-8', errors='replace')
                except Exception as e:
                    content = f"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏: {e}\n"
                out.write(content)
                out.write("\n\n")


if __name__ == '__main__':
    # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞...")
    get_project_structure(PROJECT_ROOT, STRUCTURE_FILE, IGNORE_DIRS)
    print(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{STRUCTURE_FILE}'.")

    # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞...")
    get_images_structure(PROJECT_ROOT, IMAGE_EXTENSIONS, IMAGES_STRUCTURE_FILE, IGNORE_DIRS)
    print(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{IMAGES_STRUCTURE_FILE}'.")

    # 3. –≠–∫—Å–ø–æ—Ä—Ç —Å—Ö–µ–º—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    print("–≠–∫—Å–ø–æ—Ä—Ç —Å—Ö–µ–º—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    if DATABASE_PATH.exists():
        export_database_schema(DATABASE_PATH, DATABASE_SCHEMA_FILE)
    else:
        print(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {DATABASE_PATH}")

    # 4. –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ ¬´—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö¬ª —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤–∫–ª—é—á–∏—Ç—å
    specific_files = [
        DATABASE_SCHEMA_FILE,   # schema
        IMAGES_STRUCTURE_FILE,  # images_structure.txt
        STRUCTURE_FILE,         # project_structure.txt
        DB_DESCRIPTION_FILE     # db_description.txt
    ]

    # 5. –°–æ–∑–¥–∞–Ω–∏–µ ZIP-–∞—Ä—Ö–∏–≤–∞ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ + –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ ¬´—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö¬ª —Ñ–∞–π–ª–æ–≤
    print("–°–æ–∑–¥–∞–Ω–∏–µ ZIP-–∞—Ä—Ö–∏–≤–∞ –ø—Ä–æ–µ–∫—Ç–∞...")
    create_zip_archive(
        root_dir=PROJECT_ROOT,
        archive_path=ARCHIVE_FILE,
        ignore_dirs=IGNORE_DIRS,
        ignore_files=IGNORE_FILES,
        ignore_ext=IGNORE_EXT,
        extra_files_to_add=specific_files
    )

    # 6. –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ all_codes.txt —Å–æ –≤—Å–µ–º–∏ .py —Ñ–∞–π–ª–∞–º–∏ (–∏ —Å—Ö–µ–º–æ–π –ë–î –≤ –Ω–∞—á–∞–ª–µ)
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ all_codes.txt —Å–æ –≤—Å–µ–º–∏ .py —Ñ–∞–π–ª–∞–º–∏...")
    ALL_CODES_FILE = STRUCTURE_DIR / 'all_codes.txt'
    create_all_codes_file(
        root_dir=PROJECT_ROOT,
        output_file=ALL_CODES_FILE,
        ignore_dirs=IGNORE_DIRS,
        ignore_files=IGNORE_FILES,
        ignore_ext=IGNORE_EXT,
        database_schema_file=DATABASE_SCHEMA_FILE
    )
    print(f"–§–∞–π–ª —Å –∫–æ–¥–∞–º–∏ —Å–æ–∑–¥–∞–Ω: {ALL_CODES_FILE}")

    # 7. –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    print("\n–ü—Ä–æ—Ü–µ—Å—Å –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
    print(f"–ê—Ä—Ö–∏–≤: {ARCHIVE_FILE}")
    print(f"–§–∞–π–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞: {STRUCTURE_FILE}")
    print(f"–§–∞–π–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {IMAGES_STRUCTURE_FILE}")
    print(f"–°—Ö–µ–º–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {DATABASE_SCHEMA_FILE}")
    print(f"–û–ø–∏—Å–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {DB_DESCRIPTION_FILE}")
    print(f"–í—Å–µ –∫–æ–¥—ã: {ALL_CODES_FILE}")


