====== DATABASE SCHEMA BEGIN ======

-- TABLE: user_lang
CREATE TABLE user_lang (
            user_id INTEGER PRIMARY KEY,
            lang_code TEXT NOT NULL
        );


====== DATABASE SCHEMA END ======


---- FILE: config.py ----
# ============================= FILE: config.py =============================
import os
from dotenv import load_dotenv

# Load environment variables from the .env file
load_dotenv()

# Determine the base directory of the project
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Telegram bot token retrieved from environment variables
TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")

# Path to the log file, defaults to 'bot_log.log' if not specified
LOG_FILE = os.path.join(BASE_DIR, os.getenv("LOG_FILE", "bot_log.log"))

# Directory to store audio files, defaults to 'audio' if not specified
AUDIO_FOLDER = os.getenv("AUDIO_FOLDER", "audio")

# Administrator ID retrieved from environment variables, converted to integer (default 0)
ADMIN_ID = int(os.getenv("ADMIN_ID", "0"))

# Dictionary of available languages with corresponding flags
AVAILABLE_LANGUAGES = {
    'en': {'name': 'English', 'flag': 'üá∫üá∏'},
    'ru': {'name': 'Russian', 'flag': 'üá∑üá∫'},
    'uk': {'name': 'Ukrainian', 'flag': 'üá∫üá¶'},
    'zh': {'name': 'Chinese', 'flag': 'üá®üá≥'},
}

# Maximum length of a message, used to split long logs or text
MAX_MESSAGE_LENGTH = 4000

---- FILE: test.py ----
import os
import asyncio
import subprocess
import json
import requests
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command

# ------------ –í–∞—à–∏ —Ç–æ–∫–µ–Ω—ã ------------
TELEGRAM_TOKEN = '7288388195:AAF1J1u4eTplzAtawBTT8Q6-yJkWQL--KIQ'
WIT_AI_TOKEN = 'KDWYRKLXO7KA7WUEEG3LLH5ECUVGKQE4'
# -------------------------------------

bot = Bot(token=TELEGRAM_TOKEN)
dp = Dispatcher()

# ============ –®–∞–≥ 1. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è OGG -> WAV —á–µ—Ä–µ–∑ Docker + ffmpeg =============
async def convert_to_wav(ogg_path: str, wav_path: str):
    print(f"ogg_abs_path: {os.path.abspath(ogg_path)}")
    print(f"wav_abs_path: {os.path.abspath(wav_path)}")

    command = [
        "docker", "run", "--rm",
        "-v", f"{os.getcwd()}:/data",
        "jrottenberg/ffmpeg:4.4-alpine",
        "-y",
        "-i", f"/data/{os.path.basename(ogg_path)}",
        "-acodec", "pcm_s16le",
        "-ar", "16000",
        "-ac", "1",
        "-f", "wav",
        f"/data/{os.path.basename(wav_path)}"
    ]

    print("Running command:", " ".join(command))
    proc = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print("STDOUT:", proc.stdout.decode())
    print("STDERR:", proc.stderr.decode())

    if proc.returncode != 0:
        raise Exception(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ WAV: {proc.stderr.decode()}")

# ============ –®–∞–≥ 2. –†–∞–∑—Ä–µ–∑–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ WAV –Ω–∞ –∫—É—Å–∫–∏ –ø–æ 20 —Å–µ–∫—É–Ω–¥ ============
def split_wav(wav_path: str, chunk_seconds: int = 20) -> list[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç voice.wav –Ω–∞ —Ñ–∞–π–ª—ã chunk_000.wav, chunk_001.wav, ...
    –±–µ–∑ –ø–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è (–ø—Ä–æ—Å—Ç–æ ¬´–Ω–∞—Ä–µ–∑–∫–∞¬ª).
    """

    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ chunk_*.wav
    for fname in os.listdir("."):
        if fname.startswith("chunk_") and fname.endswith(".wav"):
            os.remove(fname)

    out_templ = "chunk_%03d.wav"

    # –ö–æ–º–∞–Ω–¥–∞ ffmpeg –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    command = [
        "docker", "run", "--rm",
        "-v", f"{os.getcwd()}:/data",
        "jrottenberg/ffmpeg:4.4-alpine",
        "-i", f"/data/{os.path.basename(wav_path)}",
        "-f", "segment",
        "-segment_time", str(chunk_seconds),
        "-c", "copy",
        f"/data/{out_templ}"
    ]
    print("Split command:", " ".join(command))
    proc = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    print("SPLIT STDOUT:", proc.stdout.decode())
    print("SPLIT STDERR:", proc.stderr.decode())

    if proc.returncode != 0:
        raise Exception(f"–û—à–∏–±–∫–∞ –Ω–∞—Ä–µ–∑–∫–∏ WAV: {proc.stderr.decode()}")

    # –°–æ–±–∏—Ä–∞–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
    chunks = []
    for fname in sorted(os.listdir(".")):
        if fname.startswith("chunk_") and fname.endswith(".wav"):
            chunks.append(fname)
    return chunks

# ============ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ¬´—Å—ã—Ä–æ–≥–æ¬ª JSON-–æ—Ç–≤–µ—Ç–∞ ===========
def parse_json_objects(raw_text: str) -> list[dict]:
    """
    –ü–æ—Å–∏–º–≤–æ–ª—å–Ω–æ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã { ... } –∏–∑ –æ—Ç–≤–µ—Ç–∞ Wit.ai.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (parsed JSON).
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –±–∏—Ç—ã–µ –∫—É—Å–∫–∏, –≤—ã–≤–æ–¥–∏—Ç –æ—à–∏–±–∫–∏ –≤ print.
    """
    depth = 0
    current = ""
    result = []

    for c in raw_text:
        if c == '{':
            depth += 1
            if depth == 1:
                current = '{'
            else:
                current += '{'
        elif c == '}':
            if depth > 0:
                depth -= 1
            current += '}'
            if depth == 0:
                # –∑–∞–∫–æ–Ω—á–∏–ª–∏ –æ–±—ä–µ–∫—Ç
                try:
                    data = json.loads(current)
                    result.append(data)
                except json.JSONDecodeError as e:
                    print("[parse_json_objects] JSON decode error:", e)
                current = ""
        else:
            if depth > 0:
                current += c

    return result

# ============ –®–∞–≥ 3a. –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∫—É—Å–∫–∞ (–¥–æ 20 —Å–µ–∫—É–Ω–¥) ===============
def recognize_chunk(chunk_path: str) -> str:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ–º chunk_XXX.wav –Ω–∞ Wit.ai.
    –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≥–∏–ø–æ—Ç–µ–∑—ã.
    –ë–µ—Ä—ë–º —Ç—É, —á—Ç–æ –î–õ–ò–ù–ù–ï–ï –í–°–ï–• (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ —Å–∏–º–≤–æ–ª–æ–≤).
    """
    print(f" --- –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫—É—Å–æ–∫ {chunk_path} –≤ Wit.ai ---")
    with open(chunk_path, 'rb') as f:
        resp = requests.post(
            'https://api.wit.ai/speech?v=20220622',
            headers={
                'Authorization': f'Bearer {WIT_AI_TOKEN}',
                'Content-Type': 'audio/wav'
            },
            data=f
        )

    print(f"[{chunk_path}] Wit.ai status:", resp.status_code)
    if resp.status_code != 200:
        print(f"[{chunk_path}] –û—à–∏–±–∫–∞ Wit.ai: {resp.status_code}, {resp.text}")
        return ""

    raw_response = resp.text
    print(f"[{chunk_path}] RAW response:\n{raw_response}\n")

    # –ü–∞—Ä—Å–∏–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã
    objects = parse_json_objects(raw_response)
    print(f"[{chunk_path}] –ö–æ–ª-–≤–æ JSON –æ–±—ä–µ–∫—Ç–æ–≤:", len(objects))

    # –ò—â–µ–º ¬´—Å–∞–º—É—é –¥–ª–∏–Ω–Ω—É—é¬ª —Å—Ç—Ä–æ–∫—É text
    best_text = ""
    for obj in objects:
        txt = obj.get("text", "")
        if len(txt) > len(best_text):
            best_text = txt

    best_text = best_text.strip()
    print(f"[{chunk_path}] –õ—É—á—à–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞: '{best_text}'\n")

    return best_text

# ============ –®–∞–≥ 3b. ¬´–£–º–Ω–∞—è¬ª —Å–∫–ª–µ–π–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã —É–±—Ä–∞—Ç—å –ø–æ–≤—Ç–æ—Ä—ã ========
def smart_concat(full_text: str, new_part: str) -> str:
    """
    –ù–∞ —Å—Ç—ã–∫–µ –∫—É—Å–∫–æ–≤ —É–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä.
    –ï—Å–ª–∏ –∫–æ–Ω–µ—Ü full_text —á–∞—Å—Ç–∏—á–Ω–æ –¥—É–±–ª–∏—Ä—É–µ—Ç –Ω–∞—á–∞–ª–æ new_part ‚Äî
    –ø–æ–¥—Ä–µ–∑–∞–µ–º new_part, —á—Ç–æ–±—ã –Ω–µ –ø–ª–æ–¥–∏—Ç—å "–ø—Ä–æ–±–ª–µ–º–∞? –ø—Ä–æ–±–ª–µ–º–∞?".
    """
    full_text = full_text.strip()
    new_part = new_part.strip()

    if not new_part:
        return full_text
    if not full_text:
        return new_part

    max_overlap = min(len(full_text), len(new_part))
    overlap_size = 0

    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (full_text[-i:] == new_part[:i])
    for i in range(max_overlap, 0, -1):
        if full_text.endswith(new_part[:i]):
            overlap_size = i
            break

    tail = new_part[overlap_size:].strip()
    if tail:
        if not full_text.endswith(" "):
            full_text += " "
        full_text += tail
    return full_text

# ============ –®–∞–≥ 3 (–æ–±—â–∏–π). –†–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤–µ—Å—å WAV, —Å–∫–ª–µ–∏–≤–∞–µ–º –∫—É—Å–∫–∏ =============
async def recognize_speech(big_wav_path: str) -> str:
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    chunks = split_wav(big_wav_path, chunk_seconds=20)
    print("–ù–∞–π–¥–µ–Ω–æ –∫—É—Å–∫–æ–≤:", len(chunks))

    final_text = ""
    for idx, ch in enumerate(chunks, start=1):
        print(f"–†–∞—Å–ø–æ–∑–Ω–∞—ë–º –∫—É—Å–æ–∫ {idx}/{len(chunks)}: {ch}")
        chunk_text = recognize_chunk(ch)
        # –°–∫–ª–µ–∏–≤–∞–µ–º
        final_text = smart_concat(final_text, chunk_text)

    if not final_text.strip():
        return "–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞."
    return final_text


# ===================== –•—ç–Ω–¥–ª–µ—Ä—ã —Ç–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç–∞ =====================
@dp.message(Command('start'))
async def cmd_start(message: types.Message):
    await message.answer("–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –¥–ª–∏–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ, –∞ —è –ø–æ–ø—ã—Ç–∞—é—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –µ–≥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω–æ.")


@dp.message(F.voice)
async def handle_voice(message: types.Message):
    ogg_path = "voice.ogg"
    wav_path = "voice.wav"
    try:
        file_info = await bot.get_file(message.voice.file_id)
        await bot.download_file(file_info.file_path, ogg_path)

        # 1) OGG -> WAV
        await convert_to_wav(ogg_path, wav_path)
        # 2) –†–∞—Å–ø–æ–∑–Ω–∞—ë–º –≤—Å—ë (—Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º)
        text = await recognize_speech(wav_path)

        await message.reply(text)

    except Exception as e:
        await message.reply(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e).splitlines()[0]}")
    finally:
        # –û—á–∏—Å—Ç–∫–∞
        if os.path.exists(ogg_path):
            os.remove(ogg_path)
        if os.path.exists(wav_path):
            os.remove(wav_path)
        for fname in os.listdir("."):
            if fname.startswith("chunk_") and fname.endswith(".wav"):
                os.remove(fname)


async def main():
    await dp.start_polling(bot)

if __name__ == '__main__':
    asyncio.run(main())


---- FILE: bot.py ----
# ============================= FILE: bot.py =============================
import os
import asyncio
from aiogram import Bot, Dispatcher, Router
from aiogram.filters import Command
from aiogram.types import InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery, Message
from aiogram.fsm.storage.memory import MemoryStorage
from loguru import logger

from config import TOKEN, AUDIO_FOLDER, LOG_FILE, ADMIN_ID, AVAILABLE_LANGUAGES

# Import routers
from handlers.group_chat_handlers import group_router
from handlers.private_chat_handlers import private_router

# Import middleware
from middlewares.i18n_middleware import I18nMiddleware

from utils.text_extraction import extract_text_from_url_static, extract_text_from_url_dynamic
from utils.text_to_speech import synthesize_text_to_audio_edge
from utils.document_parsers import parse_docx, parse_fb2, parse_epub
from utils.i18n import get_translator, get_user_lang, set_user_lang

from collections import deque
from pathlib import Path


# Initialize Bot and Dispatcher
bot = Bot(token=TOKEN)
storage = MemoryStorage()
dp = Dispatcher(storage=storage)


# Register the i18n middleware
i18n = I18nMiddleware(get_translator)
dp.message.middleware(i18n)
dp.callback_query.middleware(i18n)

# Include routers
dp.include_router(private_router)
dp.include_router(group_router)

# Create the audio folder if it does not exist
audio_path = Path(AUDIO_FOLDER)
audio_path.mkdir(parents=True, exist_ok=True)

# Configure logging
logger.add(LOG_FILE, rotation="1 MB", retention="10 days", compression="zip")

# Maximum message length
MAX_MESSAGE_LENGTH = 4000

# Run the bot
async def main():
    await dp.start_polling(bot)

if __name__ == '__main__':
    asyncio.run(main())

---- FILE: main.py ----
# ============================= FILE: main.py =============================
import asyncio

from aiogram import Bot
from aiogram.types import BotCommandScopeDefault, BotCommand

from bot import dp, bot
from project_structure.paths import DATABASE_PATH
import sqlite3


async def set_bot_commands(bot: Bot):
    """
    Sets default bot commands to guide the user.
    """
    commands = [
        BotCommand(command="start", description="Start the bot"),
        BotCommand(command="help", description="Show help"),
        BotCommand(command="change_lang", description="Change language"),
    ]
    await bot.set_my_commands(commands, scope=BotCommandScopeDefault())


def init_db():
    """
    Initializes the SQLite database and creates user_lang table if it does not exist.
    """
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS user_lang (
            user_id INTEGER PRIMARY KEY,
            lang_code TEXT NOT NULL
        );
    """)
    conn.commit()
    conn.close()

async def main():
    # Initialize database
    init_db()

    # Set bot commands
    await set_bot_commands(bot)

    # Start polling updates
    await dp.start_polling(bot)

if __name__ == "__main__":
    asyncio.run(main())

---- FILE: filters/chat_type.py ----
# ============================= FILE: filters/chat_type.py =============================
from typing import Union
from aiogram.filters import BaseFilter
from aiogram.types import Message

class ChatTypeFilter(BaseFilter):
    """
    A filter to allow messages only from specified chat types (e.g., private, group, supergroup).
    """
    def __init__(self, chat_type: Union[str, list]):
        self.chat_type = chat_type

    async def __call__(self, message: Message) -> bool:
        if isinstance(self.chat_type, str):
            return message.chat.type == self.chat_type
        else:
            return message.chat.type in self.chat_type

---- FILE: middlewares/i18n_middleware.py ----
# ============================= FILE: middlewares/i18n_middleware.py =============================
from aiogram import BaseMiddleware
from aiogram.types import Message, CallbackQuery
from utils.i18n import get_translator, get_user_lang

class I18nMiddleware(BaseMiddleware):
    """
    Middleware that injects the translator function into the handler data
    based on the user's saved language.
    """
    def __init__(self, get_translator_func):
        self.get_translator = get_translator_func
        super().__init__()

    async def __call__(self, handler, event, data):
        if isinstance(event, Message):
            user_id = event.from_user.id
        elif isinstance(event, CallbackQuery):
            user_id = event.from_user.id
        else:
            user_id = None

        if user_id:
            lang_code = get_user_lang(user_id)
            translator = self.get_translator(lang_code)
        else:
            # Default language
            translator = self.get_translator('en')

        data["_"] = translator.gettext
        return await handler(event, data)

---- FILE: middlewares/concurrency_limit.py ----
# ============================= FILE: middlewares/concurrency_limit.py =============================
import asyncio
from aiogram import types
from aiogram.dispatcher.middlewares.base import BaseMiddleware
from typing import Callable, Dict, Any, Awaitable

# Import translation utilities
from utils.i18n import get_translator, get_user_lang

class ConcurrencyLimitMiddleware(BaseMiddleware):
    """
    Restricts the number of concurrent long-running tasks per user.
    """
    def __init__(self, max_concurrent_tasks: int = 1):
        super().__init__()
        self.max_concurrent_tasks = max_concurrent_tasks
        self.semaphores = {}

    def get_semaphore_for_user(self, user_id: int) -> asyncio.Semaphore:
        """
        Returns (or creates) a semaphore object for the given user_id
        to track concurrency.
        """
        if user_id not in self.semaphores:
            self.semaphores[user_id] = asyncio.Semaphore(self.max_concurrent_tasks)
        return self.semaphores[user_id]

    async def __call__(
        self,
        handler: Callable[[types.TelegramObject, Dict[str, Any]], Awaitable[Any]],
        event: types.TelegramObject,
        data: Dict[str, Any]
    ) -> Any:
        """
        Middleware handler that enforces concurrency limits.
        """
        # Check if the event is a message
        if isinstance(event, types.Message):
            user_id = event.from_user.id

            # Retrieve user's language preference
            lang_code = get_user_lang(user_id)
            translator = get_translator(lang_code)
            _ = translator.gettext  # Translation function

            semaphore = self.get_semaphore_for_user(user_id)

            if semaphore.locked():
                # Send a translatable message to the user
                await event.answer(
                    _("You already have an active task. Please wait for it to finish.")
                )
                return  # Exit without handling the message

            # Acquire semaphore and handle the message
            async with semaphore:
                return await handler(event, data)

        # For other types of events, proceed without restrictions
        return await handler(event, data)


---- FILE: middlewares/rate_limit.py ----
# ============================= FILE: middlewares/rate_limit.py =============================
import time
from aiogram import types
from aiogram.dispatcher.middlewares.base import BaseMiddleware
from typing import Callable, Dict, Any, Awaitable

# Import translation utilities
from utils.i18n import get_translator, get_user_lang

class RateLimitMiddleware(BaseMiddleware):
    """
    Simple rate limit middleware: allows 1 message per user within 'rate_limit' seconds.
    """
    def __init__(self, rate_limit: float = 2.0):
        super().__init__()
        self.rate_limit = rate_limit
        self.users_last_time = {}

    async def __call__(
        self,
        handler: Callable[[types.TelegramObject, Dict[str, Any]], Awaitable[Any]],
        event: types.TelegramObject,
        data: Dict[str, Any]
    ) -> Any:
        """
        Middleware handler that enforces rate limits.
        """
        # Check if the event is a message
        if isinstance(event, types.Message):
            user_id = event.from_user.id
            current_time = time.time()
            last_time = self.users_last_time.get(user_id, 0)

            # Retrieve user's language preference
            lang_code = get_user_lang(user_id)
            translator = get_translator(lang_code)
            _ = translator.gettext  # Translation function

            if (current_time - last_time) < self.rate_limit:
                # Send a translatable rate limit message to the user
                await event.answer(
                    _("Too many requests, please try again later (limit: {rate_limit} s).").format(rate_limit=self.rate_limit)
                )
                return  # Exit without handling the message

            # Update the last message time for the user
            self.users_last_time[user_id] = current_time

        # Proceed to handle the event
        return await handler(event, data)


---- FILE: utils/text_extraction.py ----
# ============================= FILE: utils/text_extraction.py =============================
import asyncio
from newspaper import Article
from playwright.async_api import async_playwright

async def extract_text_from_url_static(url: str) -> str:
    """
    Extract text from a URL using the Newspaper library.
    Works best for static pages.
    """
    article = Article(url)
    article.download()
    article.parse()
    return article.text.strip()

async def extract_text_from_url_dynamic(url: str) -> str:
    """
    Use Playwright to extract text from dynamically generated webpages.
    Fallback if static extraction returns too little content.
    """
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            await page.goto(url, timeout=60000)

            special_selectors = [".ReadTextContainerIn"]
            selectors = special_selectors + ["article", ".content", ".main-content"]
            article_text = None

            for selector in selectors:
                try:
                    if await page.locator(selector).count() > 0:
                        await page.wait_for_selector(selector, timeout=5000)
                        texts = await page.locator(selector).all_inner_texts()
                        article_text = "\n\n".join(texts).strip()
                        if article_text:
                            break
                except Exception:
                    continue

            if not article_text or article_text.strip() == "":
                article_text = await page.evaluate("document.body.innerText")

            return article_text.strip()
        except Exception:
            return ""
        finally:
            await browser.close()

---- FILE: utils/text_analysis.py ----
# ============================= FILE: utils/text_analysis.py =============================
import re
from collections import Counter
import textstat
import nltk
from rake_nltk import Rake

nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)

# Stop words in Russian plus some additional ones
STOP_WORDS = set(nltk.corpus.stopwords.words('russian')).union({
    "–≤", "–Ω–∞", "–∫", "–ø–æ", "—Å", "–¥–ª—è", "–∏", "–∏–ª–∏", "–æ—Ç", "–∏–∑", "–∑–∞", "–æ", "–æ–±",
    "—á—Ç–æ", "–Ω–µ", "—ç—Ç–æ", "–Ω–æ", "—Ç–∞–∫", "–∂–µ", "–∫–∞–∫", "–∫–æ–≥–¥–∞", "–µ—Å–ª–∏", "–≥–¥–µ", "–∫—Ç–æ",
    "–º–æ–∂–Ω–æ", "—Ç–æ–ª—å–∫–æ", "–±—É–¥–µ—Ç", "–ø—Ä–∏", "–∏–∑-–∑–∞", "–ø–æ—Ç–æ–º—É", "—Ç–æ–≥–¥–∞", "–≤–æ", "–±—ã",
    "—Ç–∞–º", "—Å—Ä–∞–∑—É", "–ø–æ–∫–∞", "–ª–∏", "—á—Ç–æ–±—ã", "—Å–µ–π—á–∞—Å", "–µ—â—ë", "–º–µ–∂–¥—É", "–¥–∞–∂–µ",
    "–º–æ–∂–µ—Ç", "–ø–æ—Å–ª–µ", "–ø–µ—Ä–µ–¥", "–ø—Ä–∏", "—Ç—É—Ç", "–¥–∞"
})

async def analyze_text(text: str, _: callable) -> str:
    """
    Analyzes text:
    - Word/character counts
    - Estimated reading time
    - Top-5 most common words (excluding stopwords)
    - Reading difficulty (Flesch Reading Ease)
    - Education level required
    - Key phrases via RAKE

    Returns a formatted summary string in the user's language via _() for i18n.
    """
    word_count = len(text.split())
    char_count = len(text)
    estimated_time_seconds = round(len(text) / 150)
    estimated_time_minutes = estimated_time_seconds // 60
    estimated_time_remainder_seconds = estimated_time_seconds % 60

    estimated_time_str = ""
    if estimated_time_minutes > 0:
        estimated_time_str += _("{minutes} min ").format(minutes=estimated_time_minutes)
    if estimated_time_remainder_seconds > 0 or estimated_time_minutes == 0:
        estimated_time_str += _("{seconds} sec").format(seconds=estimated_time_remainder_seconds)

    words = re.findall(r'\b\w+\b', text.lower())
    filtered_words = [word for word in words if word not in STOP_WORDS]
    common_words = Counter(filtered_words).most_common(5)

    reading_ease = textstat.flesch_reading_ease(text)
    grade_level = textstat.text_standard(text, float_output=False)

    if reading_ease > 80:
        reading_level = _("Very easy to read")
    elif reading_ease > 60:
        reading_level = _("Easy to read")
    elif reading_ease > 40:
        reading_level = _("Moderately difficult")
    elif reading_ease > 20:
        reading_level = _("Hard to read")
    else:
        reading_level = _("Very hard to read")

    rake = Rake()
    rake.extract_keywords_from_text(text)
    key_phrases = rake.get_ranked_phrases()[:5]

    summary_of_the_text = _(
        "üìù Your text contains {word_count} words and {char_count} characters.\n"
        "‚è≥ Approximate narration time: {estimated_time_str}.\n\n"
        "üìä <b>Text Analysis</b>:\n\n"
        "- <b>Top-5 words</b>: {top_words}\n"
        "- <b>Reading level</b>: {reading_level} (Flesch: {reading_ease:.2f})\n"
        "- <b>Suggested education level</b>: {grade_level}\n\n"
        "- <b>Key phrases</b>: {key_phrases}\n"
    ).format(
        word_count=word_count,
        char_count=char_count,
        estimated_time_str=estimated_time_str,
        top_words=', '.join([_("{word} ({count})").format(word=w, count=c) for w, c in common_words]),
        reading_level=reading_level,
        reading_ease=reading_ease,
        grade_level=grade_level,
        key_phrases=', '.join(key_phrases)
    )

    return summary_of_the_text

---- FILE: utils/document_parsers.py ----
# ============================= FILE: utils/document_parsers.py =============================
from xml.etree import ElementTree
from docx import Document
from ebooklib import epub, ITEM_DOCUMENT
from bs4 import BeautifulSoup
import warnings
from loguru import logger

def parse_docx(file_path: str) -> str:
    """
    Parse a .docx file into text.
    """
    try:
        doc = Document(file_path)
        full_text = []
        for para in doc.paragraphs:
            paragraph_text = para.text.strip()
            if paragraph_text:
                full_text.append(paragraph_text)
        extracted_text = "\n".join(full_text)
        words_count = len(extracted_text.split())
        logger.info(f"[DOCX] Extracted {words_count} words from {file_path}")
        return extracted_text
    except Exception as e:
        logger.error(f"Error parsing DOCX {file_path}: {e}")
        return ""

def parse_fb2(file_path: str) -> str:
    """
    Parse an .fb2 file into text.
    """
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        data_str = data.decode('utf-8', errors='replace')
        root = ElementTree.fromstring(data_str)
        ns = root.tag.split('}')[0].strip('{')
        namespaces = {'fb2': ns}

        sections = root.findall('.//fb2:section', namespaces=namespaces)
        text_chunks = []
        for sec in sections:
            paragraphs = sec.findall('.//fb2:p', namespaces=namespaces)
            sec_text = "\n".join([p.text.strip() for p in paragraphs if p.text])
            if sec_text.strip():
                text_chunks.append(sec_text)
        extracted_text = "\n\n".join(text_chunks).strip()
        words_count = len(extracted_text.split())
        logger.info(f"[FB2] Extracted {words_count} words from {file_path}")
        return extracted_text
    except Exception as e:
        logger.error(f"Error parsing FB2 {file_path}: {e}")
        return ""

def parse_epub(file_path: str) -> str:
    """
    Parse an .epub file into text.
    """
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            book = epub.read_epub(file_path)
        items = list(book.get_items_of_type(ITEM_DOCUMENT))
        logger.info(f"EPUB has {len(items)} document items")
        full_text = []

        for idx, item in enumerate(items, start=1):
            if item.get_type() == ITEM_DOCUMENT:
                content = item.get_content().decode('utf-8', errors='ignore')
                soup = BeautifulSoup(content, 'html.parser')
                for bad_tag in soup(["script", "style", "nav", "header", "footer", "meta", "link"]):
                    bad_tag.extract()
                paragraphs = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                text_only = "\n".join(
                    para.get_text(separator=' ', strip=True) for para in paragraphs
                ).strip()

                logger.debug(f"[EPUB] Part {idx}: extracted {len(text_only.split())} words.")
                if text_only:
                    full_text.append(text_only)

        extracted_text = "\n".join(full_text)
        total_words = len(extracted_text.split())
        logger.info(f"[EPUB] Total extracted words: {total_words}")
        return extracted_text
    except Exception as e:
        logger.error(f"Error parsing EPUB {file_path}: {e}")
        return ""

---- FILE: utils/text_to_speech.py ----
# ============================= FILE: utils/text_to_speech.py =============================
import time
import os
import edge_tts
from aiogram.types import Message, FSInputFile
from loguru import logger
from config import AUDIO_FOLDER
from .text_analysis import analyze_text
from langdetect import detect, DetectorFactory, LangDetectException
import re
import uuid  # for generating unique identifiers

DetectorFactory.seed = 0

CHUNK_SIZE = 40000

VOICE_MAP = {
    "ru": "ru-RU-DmitryNeural",
    "en": "en-US-JennyNeural",
    "uk": "uk-UA-OstapNeural",
    "zh-cn": "zh-CN-XiaoxiaoNeural",
}

def detect_language(text: str) -> str:
    """
    Detects the language of the text.
    Returns language code (e.g., 'ru', 'en', 'uk', 'zh-cn').
    If detection fails, returns 'unknown'.
    """
    try:
        lang = detect(text)
        lang = lang.lower()
        if lang.startswith("zh"):
            return "zh-cn"
        return lang
    except LangDetectException:
        return "unknown"

async def chunk_text(text: str, chunk_size: int = CHUNK_SIZE):
    """
    Splits text into parts of up to chunk_size characters.
    """
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += chunk_size
    return chunks

async def synthesize_chunk(text: str, mp3_path: str,
                           voice: str = "ru-RU-DmitryNeural",
                           rate: str = "+50%"):
    """
    Synthesizes a single chunk of text and saves it to mp3_path using Edge TTS.
    """
    communicate = edge_tts.Communicate(text=text, voice=voice, rate=rate)
    await communicate.save(mp3_path)

def sanitize_filename(text: str) -> str:
    """
    Sanitizes the input text to create a valid filename.
    Removes or replaces characters that are not allowed in filenames.
    """
    # Remove invalid characters
    sanitized = re.sub(r'[<>:"/\\|?*\x00-\x1F]', '', text)
    # Replace spaces with underscores
    sanitized = re.sub(r'\s+', '_', sanitized)
    # Limit the filename length to 50 characters
    return sanitized[:50] if len(sanitized) > 50 else sanitized

async def synthesize_text_to_audio_edge(text: str, filename_prefix: str, message: Message, logger, _):
    """
    Synthesizes large text by parts and sends each part to the user immediately:
    1. Analyze text and send summary.
    2. Detect language and pick a corresponding voice.
    3. Split text into parts.
    4. Synthesize and send each part, then remove the file.
    """
    if not text.strip():
        logger.warning(_("Empty text for synthesis."))
        await message.reply(_("Could not synthesize: empty text."))
        return

    if len(text) > 1000000:
        logger.info(_("The text is too large: {len_text} characters").format(len_text=len(text)))
        await message.reply(_("The text is too large. Contact @maksenro if you want to increase limits!"))
        return

    logger.info(_("@{username}, length - {len_text}: {preview}...").format(
        username=message.from_user.username if message.from_user.username else "User",
        len_text=len(text),
        preview=text[:100]
    ))
    summary = await analyze_text(text, _)
    await message.reply(summary, parse_mode='HTML')

    lang = detect_language(text)
    if lang == "unknown":
        logger.warning(_("Could not detect language. Using default voice."))
        voice = "ru-RU-DmitryNeural"
        await message.reply(_("Could not detect language. Using default voice."))
    else:
        voice = VOICE_MAP.get(lang, "ru-RU-DmitryNeural")
        logger.info(_("Detected language: {lang}. Using voice: {voice}").format(lang=lang, voice=voice))

    # Create base filename by sanitizing the first 25 characters of the text
    base_text = text[:25]
    sanitized_base = sanitize_filename(base_text)
    base_filename = f"{sanitized_base}_{filename_prefix}"
    parts = await chunk_text(text)
    total_parts = len(parts)

    for i, chunked_text in enumerate(parts, start=1):
        # Generate a unique filename using UUID for extra uniqueness
        unique_id = uuid.uuid4().hex
        if total_parts > 1:
            part_filename = f"{i}_of_{total_parts}_{base_filename}_{unique_id}.mp3"
        else:
            part_filename = f"{base_filename}_{unique_id}.mp3"
        mp3_path = os.path.join(AUDIO_FOLDER, part_filename)

        start_time = time.time()
        try:
            await synthesize_chunk(chunked_text, mp3_path, voice=voice)
            end_time = time.time()

            logger.debug(_("Part {i}/{total_parts} synthesized in {seconds} sec.").format(
                i=i,
                total_parts=total_parts,
                seconds=round(end_time - start_time, 2)
            ))

            audio_file = FSInputFile(mp3_path)
            await message.reply_audio(audio=audio_file)
            logger.info(f"Sent audio part {i}/{total_parts} to user {message.from_user.id}.")
        except Exception as e:
            logger.error(f"Error synthesizing part {i}/{total_parts}: {e}")
            await message.reply(_("Failed to synthesize part {i}: {error}").format(i=i, error=str(e)))
        finally:
            # Remove the file if it exists
            if os.path.exists(mp3_path):
                os.remove(mp3_path)
                logger.info(f"Removed temporary audio file {mp3_path}.")

---- FILE: utils/i18n.py ----
# ============================= FILE: utils/i18n.py =============================
import gettext
import os
import sqlite3
from project_structure.paths import DATABASE_PATH, PROJECT_ROOT

LOCALES_DIR = PROJECT_ROOT / "locales"

def get_translator(lang_code: str):
    """
    Returns a gettext translation object for the specified lang_code.
    If not found, defaults to 'en'.
    """
    if lang_code not in ['en', 'ru', 'uk', 'zh']:
        lang_code = 'en'
    try:
        return gettext.translation(
            domain='messages',
            localedir=LOCALES_DIR,
            languages=[lang_code]
        )
    except FileNotFoundError:
        return gettext.translation(
            domain='messages',
            localedir=LOCALES_DIR,
            languages=['en']
        )

def get_user_lang(user_id: int) -> str:
    """
    Retrieves the saved language code for a specific user from the database.
    Defaults to 'ru' if not found or on error.
    """
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        cursor.execute("SELECT lang_code FROM user_lang WHERE user_id = ?", (user_id,))
        row = cursor.fetchone()
        conn.close()
        if row:
            return row[0]
        else:
            return "ru"
    except:
        return "ru"

def set_user_lang(user_id: int, lang_code: str):
    """
    Updates or inserts the language code for a user in the database.
    """
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO user_lang (user_id, lang_code)
        VALUES (?, ?)
        ON CONFLICT(user_id) DO UPDATE SET lang_code=excluded.lang_code
    """, (user_id, lang_code))
    conn.commit()
    conn.close()

---- FILE: project_structure/paths.py ----
# ============================= FILE: project_structure/paths.py =============================
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent.parent
DATABASE_PATH = PROJECT_ROOT / 'MKvoiceDB.sqlite'

STRUCTURE_DIR = PROJECT_ROOT / 'project_structure' / 'backup'
STRUCTURE_DIR.mkdir(parents=True, exist_ok=True)

STRUCTURE_FILE = STRUCTURE_DIR / 'project_structure.txt'
IMAGES_STRUCTURE_FILE = STRUCTURE_DIR / 'images_structure.txt'
ARCHIVE_FILE = STRUCTURE_DIR / 'project_archive.zip'
DATABASE_SCHEMA_FILE = STRUCTURE_DIR / 'database_schema_telegram_messages.sql'
DB_DESCRIPTION_FILE = STRUCTURE_DIR / 'db_description.txt'

IGNORE_DIRS = {
    '.git', '.idea', '__pycache__', 'venv', 'env',
    '.venv', '.env', 'node_modules', 'backup'
}
IGNORE_FILES = {'.DS_Store', 'project_archive.zip'}
IMAGE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.svg', '.webp'}
IGNORE_EXT = {'.pyc', '.pyo', '.log', '.db', '.zip', '.tar', '.gz', '.bz2', '.7z', '.rar'}

LOG_DIR = PROJECT_ROOT / 'logs'
LOG_DIR.mkdir(exist_ok=True)

---- FILE: project_structure/project_structure_creator.py ----
# ============================= FILE: project_structure/project_structure_creator.py =============================
import zipfile
import sqlite3
import os
from pathlib import Path

from project_structure.paths import (
    PROJECT_ROOT,
    STRUCTURE_DIR,
    STRUCTURE_FILE,
    IMAGES_STRUCTURE_FILE,
    ARCHIVE_FILE,
    IGNORE_DIRS,
    IGNORE_FILES,
    IMAGE_EXTENSIONS,
    IGNORE_EXT,
    DATABASE_SCHEMA_FILE,
    DB_DESCRIPTION_FILE,
    DATABASE_PATH
)

def should_ignore(file_path: Path, relative_path: Path, ignore_dirs, ignore_files, ignore_ext) -> bool:
    """
    Checks if the file/dir should be ignored during archiving or other operations.
    """
    for ignore_dir in ignore_dirs:
        ignore_dir_path = Path(ignore_dir)
        if ignore_dir_path in relative_path.parents:
            print(f"Ignoring {relative_path} due to ignored directory {ignore_dir}")
            return True

    if relative_path.name in ignore_files:
        print(f"Ignoring file {relative_path} due to ignored name")
        return True

    if relative_path.suffix.lower() in ignore_ext:
        print(f"Ignoring file {relative_path} due to ignored extension")
        return True

    return False

def get_project_structure(root_dir: Path, output_file: Path, ignore_dirs):
    """
    Recursively walk the project structure and write the directory tree to output_file.
    """
    with output_file.open('w', encoding='utf-8') as f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            if relative_dir == Path('.'):
                level = 0
            else:
                level = len(relative_dir.parts)
            indent = '    ' * level
            f.write(f"{indent}{current_dir.name}/\n")

            subindent = '    ' * (level + 1)
            for name in filenames:
                f.write(f"{subindent}{name}\n")


def get_images_structure(root_dir: Path, image_extensions, output_file: Path, ignore_dirs):
    """
    Recursively walk the project and write the tree of images to output_file.
    """
    with output_file.open('w', encoding='utf-8') as f:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            image_files = [
                file for file in filenames
                if (current_dir / file).suffix.lower() in image_extensions
            ]
            if image_files:
                if relative_dir == Path('.'):
                    level = 0
                else:
                    level = len(relative_dir.parts)
                indent = '    ' * level
                f.write(f"{indent}{current_dir.name}/\n")
                subindent = '    ' * (level + 1)
                for name in image_files:
                    f.write(f"{subindent}{name}\n")


def export_database_schema(db_path: Path, output_file: Path):
    """
    Exports the full database schema including tables, indexes, views, and triggers.
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        with output_file.open('w', encoding='utf-8') as f:
            cursor.execute("SELECT type, name, sql FROM sqlite_master WHERE sql IS NOT NULL;")
            objects = cursor.fetchall()

            for obj_type, obj_name, obj_sql in objects:
                f.write(f"-- {obj_type.upper()}: {obj_name}\n")
                f.write(f"{obj_sql};\n\n")

            print(f"Database schema exported to '{output_file}'.")
        conn.close()
    except Exception as e:
        print(f"Error exporting database schema: {e}")


def create_zip_archive(
    root_dir: Path,
    archive_path: Path,
    ignore_dirs,
    ignore_files,
    ignore_ext,
    extra_files_to_add=None
):
    """
    Creates a zip archive of the entire project, ignoring unneeded files/folders.
    Also adds any extra_files_to_add to the archive if needed.
    """
    added_rel_paths = set()

    with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            for filename in filenames:
                file_path = current_dir / filename
                rel_path = file_path.relative_to(root_dir)

                if should_ignore(file_path, rel_path, ignore_dirs, ignore_files, ignore_ext):
                    continue

                rel_path_str = str(rel_path).replace(os.sep, '/')
                if rel_path_str not in added_rel_paths:
                    zipf.write(file_path, arcname=rel_path_str)
                    added_rel_paths.add(rel_path_str)
                else:
                    print(f"File '{rel_path}' already in archive, skipping.")

        if extra_files_to_add:
            for special_file in extra_files_to_add:
                if not special_file.exists() or not special_file.is_file():
                    print(f"File '{special_file}' not found, skipping.")
                    continue

                try:
                    arcname = special_file.relative_to(root_dir)
                except ValueError:
                    arcname = special_file.name

                arcname_str = str(arcname).replace(os.sep, '/')
                if arcname_str not in added_rel_paths:
                    zipf.write(special_file, arcname=arcname_str)
                    added_rel_paths.add(arcname_str)
                    print(f"Added file '{special_file}' to archive.")
                else:
                    print(f"File '{special_file}' already in archive, skipping.")
    print(f"Archive '{archive_path}' created successfully.")


def create_all_codes_file(
    root_dir: Path,
    output_file: Path,
    ignore_dirs,
    ignore_files,
    ignore_ext,
    database_schema_file: Path = None
):
    """
    Creates a file all_codes.txt containing:
    1. The database schema (if available).
    2. The content of all non-ignored .py files with a heading.
    """
    with output_file.open('w', encoding='utf-8') as out:
        if database_schema_file and database_schema_file.exists():
            db_schema_content = database_schema_file.read_text(encoding='utf-8', errors='replace')
            out.write("====== DATABASE SCHEMA BEGIN ======\n\n")
            out.write(db_schema_content)
            out.write("\n====== DATABASE SCHEMA END ======\n\n\n")
        else:
            out.write("-- Database schema is missing or was not exported --\n\n")

        for dirpath, dirnames, filenames in os.walk(root_dir):
            current_dir = Path(dirpath)
            try:
                relative_dir = current_dir.relative_to(root_dir)
            except ValueError:
                continue

            dirnames[:] = [
                d for d in dirnames
                if d not in ignore_dirs
            ]

            for filename in filenames:
                file_path = current_dir / filename
                rel_path = file_path.relative_to(root_dir)

                if should_ignore(file_path, rel_path, ignore_dirs, ignore_files, ignore_ext):
                    continue

                if file_path.suffix.lower() != '.py':
                    continue

                out.write(f"---- FILE: {rel_path} ----\n")
                try:
                    content = file_path.read_text(encoding='utf-8', errors='replace')
                except Exception as e:
                    content = f"Cannot read file due to error: {e}\n"
                out.write(content)
                out.write("\n\n")


if __name__ == '__main__':
    print("Generating project structure...")
    get_project_structure(PROJECT_ROOT, STRUCTURE_FILE, IGNORE_DIRS)
    print(f"Project structure saved to '{STRUCTURE_FILE}'.")

    print("Generating project images structure...")
    get_images_structure(PROJECT_ROOT, IMAGE_EXTENSIONS, IMAGES_STRUCTURE_FILE, IGNORE_DIRS)
    print(f"Images structure saved to '{IMAGES_STRUCTURE_FILE}'.")

    print("Exporting database schema...")
    if DATABASE_PATH.exists():
        export_database_schema(DATABASE_PATH, DATABASE_SCHEMA_FILE)
    else:
        print(f"Database not found at: {DATABASE_PATH}")

    specific_files = [
        DATABASE_SCHEMA_FILE,
        IMAGES_STRUCTURE_FILE,
        STRUCTURE_FILE,
        DB_DESCRIPTION_FILE
    ]

    print("Creating ZIP archive of the project...")
    create_zip_archive(
        root_dir=PROJECT_ROOT,
        archive_path=ARCHIVE_FILE,
        ignore_dirs=IGNORE_DIRS,
        ignore_files=IGNORE_FILES,
        ignore_ext=IGNORE_EXT,
        extra_files_to_add=specific_files
    )

    print("Creating all_codes.txt with all .py files...")
    ALL_CODES_FILE = STRUCTURE_DIR / 'all_codes.txt'
    create_all_codes_file(
        root_dir=PROJECT_ROOT,
        output_file=ALL_CODES_FILE,
        ignore_dirs=IGNORE_DIRS,
        ignore_files=IGNORE_FILES,
        ignore_ext=IGNORE_EXT,
        database_schema_file=DATABASE_SCHEMA_FILE
    )
    print(f"Codes file created: {ALL_CODES_FILE}")

    print("\nProcess finished successfully!")
    print(f"Archive: {ARCHIVE_FILE}")
    print(f"Project structure file: {STRUCTURE_FILE}")
    print(f"Images structure file: {IMAGES_STRUCTURE_FILE}")
    print(f"Database schema: {DATABASE_SCHEMA_FILE}")
    print(f"Database description: {DB_DESCRIPTION_FILE}")
    print(f"All codes: {ALL_CODES_FILE}")

---- FILE: handlers/private_chat_handlers.py ----
# ============================= FILE: handlers/private_chat_handlers.py =============================
import asyncio
import os

import chardet
from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import Message, InlineKeyboardMarkup, InlineKeyboardButton, CallbackQuery
from loguru import logger

from filters.chat_type import ChatTypeFilter
from utils.text_extraction import extract_text_from_url_static, extract_text_from_url_dynamic
from utils.text_to_speech import synthesize_text_to_audio_edge
from utils.document_parsers import parse_docx, parse_fb2, parse_epub
from utils.i18n import get_translator, get_user_lang, set_user_lang

from collections import deque
from pathlib import Path

from config import AUDIO_FOLDER, LOG_FILE, ADMIN_ID, AVAILABLE_LANGUAGES, MAX_MESSAGE_LENGTH

# Create a router for private chats
private_router = Router()
private_router.message.filter(ChatTypeFilter(chat_type=["private"]))

@private_router.message(Command('start'))
async def cmd_start(message: Message, _: callable):
    """
    Sends a greeting and instructions for the bot in a private chat.
    """
    user_id = message.from_user.id
    lang_code = get_user_lang(user_id)
    translator = get_translator(lang_code)
    __ = translator.gettext

    logger.info(f"User {user_id} started the bot in a private chat.")
    help_text = _(
        "üëã Hi! I can help you convert text to speech in various ways.\n\n"
        "üîπ <b>What I can do:</b>\n"
        "- Send me a text message to synthesize speech.\n"
        "- Send a link to a website to extract text and synthesize it.\n"
        "- Send a file (<code>.docx</code>, <code>.fb2</code>, <code>.epub</code>) to extract text and synthesize it.\n"
        "- Add me to a group chat and use the command <b>/vv help</b> to see how to work with me in groups.\n\n"
        "üìÑ <b>Available commands:</b>\n"
        "<b>/help</b> - Show help\n"
        "<b>/change_lang</b> - Change interface language\n\n"
        "‚ùì <b>Questions or suggestions?</b>\n"
        "Contact the bot administrator @maksenro.\n\n"
        "[ <a href=\"https://www.donationalerts.com/r/mkprod\">Support</a> | <a href=\"https://t.me/MKprodaction\">Group</a> ]"
    )
    await message.answer(help_text, parse_mode='HTML')

@private_router.message(Command('change_lang'))
async def cmd_change_lang(message: Message, _: callable):
    """
    Sends an inline keyboard with available languages for the user to choose from.
    """
    keyboard = InlineKeyboardMarkup(
        inline_keyboard=[
            [
                InlineKeyboardButton(
                    text=f"{lang_info['flag']} {lang_info['name']}",
                    callback_data=f"change_lang:{lang_code}"
                )
            ] for lang_code, lang_info in AVAILABLE_LANGUAGES.items()
        ]
    )
    logger.info(f"User {message.from_user.id} requested to change language.")
    await message.reply(_("Please choose your language:"), reply_markup=keyboard)

@private_router.callback_query(F.data.startswith("change_lang:"))
async def process_change_lang(callback_query: CallbackQuery, _: callable):
    """
    Processes the language change callback. Updates the user's language in the DB.
    """
    lang_code = callback_query.data.split(":")[1]

    if lang_code not in AVAILABLE_LANGUAGES:
        await callback_query.answer(_("Unsupported language."), show_alert=True)
        logger.warning(f"User {callback_query.from_user.id} tried to set unsupported language: {lang_code}")
        return

    set_user_lang(callback_query.from_user.id, lang_code)
    translator = get_translator(lang_code)
    __ = translator.gettext

    await callback_query.answer()
    await callback_query.message.edit_reply_markup()
    await callback_query.message.reply(_("Language updated!"))
    logger.info(f"User {callback_query.from_user.id} updated language to {lang_code}.")

@private_router.message(Command('help'))
async def cmd_help(message: Message, _: callable):
    """
    Sends a help message describing the available commands and bot features.
    """
    help_text = _(
        "üìñ <b>Available commands:</b>\n"
        "/start - Begin interaction with the bot\n"
        "/help - Show this message\n"
        "/change_lang - Change interface language\n\n"
        "ü§ñ <b>Bot capabilities:</b>\n"
        "- Send text messages, and the bot will synthesize them using text-to-speech.\n"
        "- Send documents in <code>.docx</code>, <code>.fb2</code>, <code>.epub</code> formats, and the bot will extract and synthesize them.\n"
        "- Send links to web pages, and the bot will extract and synthesize the text.\n\n"
        "üìÇ <b>Supported formats:</b>\n"
        "- Text messages: any text\n"
        "- Documents: <code>.docx</code>, <code>.fb2</code>, <code>.epub</code>\n"
        "- Links: HTTP/HTTPS\n\n"
        "‚öôÔ∏è <b>Settings:</b>\n"
        "- Use <b>/change_lang</b> command to change the interface language.\n"
        "- The bot supports multiple languages: English, Russian, Ukrainian, Chinese.\n\n"
        "üë• <b>Use in groups:</b>\n"
        "- Add the bot to a group.\n"
        "- Grant it the following permissions:\n"
        "  ‚Ä¢ Read messages\n"
        "  ‚Ä¢ Send messages\n"
        "  ‚Ä¢ Manage messages\n\n"
        "- <b>Available commands in groups:</b>\n"
        "  ‚Ä¢ /vv &lt;text&gt; - Synthesize the provided text.\n"
        "  ‚Ä¢ /vv &lt;link&gt; - Extract text from the link and synthesize it.\n"
        "  ‚Ä¢ /vv (in reply to a message) - Synthesize text from the replied-to message.\n\n"
        "‚ùì <b>Questions or suggestions?</b>\n"
        "Contact the bot administrator @maksenro.\n\n"
        "[ <a href=\"https://www.donationalerts.com/r/mkprod\">Support</a> | <a href=\"https://t.me/MKprodaction\">Group</a> ]"
    )

    await message.answer(help_text, parse_mode='HTML')

@private_router.message(Command(commands=["s", "S", "—ã", "–´"]))
async def cmd_s(message: Message, _: callable):
    """
    An admin-only command to retrieve the last lines of the log file.
    """
    if message.from_user.id != ADMIN_ID:
        logger.warning(f"Access denied for user {message.from_user.id} to command 's'.")
        await message.reply(_("Access denied."))
        return

    try:
        log_file = Path(LOG_FILE)
        if not log_file.exists():
            logger.error(f"Log file {LOG_FILE} does not exist.")
            await message.reply(_("Log file does not exist."))
            return

        with log_file.open('r', encoding='utf-8') as f:
            last_n_lines = deque(f, 15)
        last_lines = ''.join(last_n_lines)

        if not last_lines.strip():
            await message.reply(_("No log messages."))
            return

        messages_list = []
        current_message = ""
        for line in last_lines.splitlines(keepends=True):
            if len(current_message) + len(line) > MAX_MESSAGE_LENGTH:
                messages_list.append(current_message)
                current_message = line
            else:
                current_message += line
        if current_message:
            messages_list.append(current_message)

        for msg in messages_list:
            await message.reply(_("üìù Last log lines:\n") + msg, parse_mode='HTML')
            await asyncio.sleep(0.1)
        logger.info(f"Admin {message.from_user.id} retrieved last log lines.")

    except Exception as e:
        logger.error(f"Failed to read log file: {e}")
        await message.reply(_("Failed to read log file: {error}").format(error=str(e)))

@private_router.message(F.text.regexp(r'^https?://'))
async def handle_url(message: Message, _: callable):
    """
    Handles a URL in a private chat. Tries static extraction,
    then dynamic extraction, then synthesizes the extracted text.
    """
    url = message.text
    try:
        text_page = await extract_text_from_url_static(url)
        if len(text_page) < 200:
            text_page = await extract_text_from_url_dynamic(url)
        if not text_page.strip():
            await message.reply(_("Could not extract text."))
            logger.warning(f"User {message.from_user.id} sent URL but no text extracted.")
            return

        await synthesize_text_to_audio_edge(
            text_page,
            str(message.from_user.id),
            message,
            logger,
            _
        )
        logger.info(f"Processed URL from user {message.from_user.id}: {url}")

    except Exception as e:
        await message.reply(_("Failed to process URL: {error}").format(error=str(e)))
        logger.error(f"Failed to process URL from user {message.from_user.id}: {url} | Error: {e}")

@private_router.message(F.text)
async def handle_text(message: Message, _: callable):
    """
    Handles any plain text sent by the user. Synthesizes the text to speech.
    """
    text = message.text
    if not text.strip():
        await message.reply(_("Empty text."))
        logger.warning(f"User {message.from_user.id} sent empty text.")
        return

    await synthesize_text_to_audio_edge(text, str(message.from_user.id), message, logger, _)
    logger.info(f"Voiced text from user {message.from_user.id}: {text}")

@private_router.message(F.document)
async def handle_file(message: Message, _: callable):
    """
    Handles documents in private chat. Accepts .docx, .fb2, .epub, or text with encoding detection.
    Extracts text and synthesizes it.
    """
    if message.document.file_size > 20 * 1024 * 1024:
        await message.reply(_("File is too large (max 20 MB)."))
        logger.warning(f"User {message.from_user.id} sent a file that is too large: {message.document.file_name}")
        return

    file_extension = os.path.splitext(message.document.file_name)[1].lower()
    local_file_path = os.path.join(AUDIO_FOLDER, message.document.file_name)

    try:
        # Download file
        await message.document.download(destination_file=local_file_path)
        logger.info(f"Downloaded file from user {message.from_user.id}: {message.document.file_name}")

        # Detect encoding
        with open(local_file_path, 'rb') as f:
            raw_data = f.read()
        detected = chardet.detect(raw_data)
        encoding = detected['encoding']
        confidence = detected['confidence']
        logger.info(f"Detected encoding for {message.document.file_name}: {encoding} with confidence {confidence}")

        # Extract text
        if file_extension == ".docx":
            text = parse_docx(local_file_path)
        elif file_extension == ".fb2":
            text = parse_fb2(local_file_path)
        elif file_extension == ".epub":
            text = parse_epub(local_file_path)
        else:
            if encoding is None:
                encoding = 'utf-8'
            with open(local_file_path, "r", encoding=encoding, errors='replace') as txt_f:
                text = txt_f.read()

        # Remove file from local storage
        os.remove(local_file_path)
        logger.info(f"Processed and removed file {local_file_path}")

        if not text.strip():
            await message.reply(_("Could not extract text from the document."))
            logger.warning(f"No text extracted from document {message.document.file_name} by user {message.from_user.id}.")
            return

        # Synthesize text
        await synthesize_text_to_audio_edge(text, str(message.from_user.id), message, logger, _)
        logger.info(f"Synthesized document for user {message.from_user.id}: {message.document.file_name}")

    except Exception as e:
        logger.error(f"Failed to process document from user {message.from_user.id}: {e}")
        await message.reply(_("Failed to process document: {error}").format(error=str(e)))
        if os.path.exists(local_file_path):
            os.remove(local_file_path)
            logger.info(f"Removed corrupted file {local_file_path}")

---- FILE: handlers/group_chat_handlers.py ----
# ============================= FILE: handlers/group_chat_handlers.py =============================
import os
from aiogram import Router, F
from aiogram.filters import Command
from aiogram.types import Message
from loguru import logger

from filters.chat_type import ChatTypeFilter
from utils.text_extraction import extract_text_from_url_static, extract_text_from_url_dynamic
from utils.text_to_speech import synthesize_text_to_audio_edge

from config import AVAILABLE_LANGUAGES, MAX_MESSAGE_LENGTH

import chardet  # We might need this for group file handling if implemented.

# Create a router for group chats
group_router = Router()
group_router.message.filter(ChatTypeFilter(chat_type=["group", "supergroup"]))

@group_router.message(F.text.regexp(r'^\/vv\s+(help|h)\s*$'))
async def cmd_vv_help_in_group(message: Message, _: callable):
    """
    Sends usage help for the /vv command in group chats.
    """
    help_text = _(
        "üìñ <b>Help for /vv command:</b>\n\n"
        "üîπ <b>/vv &lt;text&gt;</b> - Synthesize the provided text.\n"
        "üîπ <b>/vv &lt;link&gt;</b> - Extract text from the link and synthesize it.\n"
        "üîπ <b>/vv</b> (in reply to a message) - Synthesize text from the replied-to message.\n"
        "üîπ <b>/vv help</b> or <b>/vv h</b> - Shows this help.\n\n"
        "üìÇ <b>Supported formats:</b>\n"
        "- Text: any text\n"
        "- Links: HTTP/HTTPS\n\n"
        "‚öôÔ∏è <b>Settings:</b>\n"
        "- The bot supports multiple languages: English, Russian, Ukrainian, Chinese.\n\n"
        "‚ùì <b>Questions or suggestions?</b>\n"
        "Contact the bot administrator @maksenro.\n\n"
        "[ @MKttsBOT | <a href=\"https://www.donationalerts.com/r/mkprod\">Support</a> | <a href=\"https://t.me/MKprodaction\">Group</a> ]"
    )
    logger.info(f"Group {message.chat.id} requested /vv help.")
    await message.reply(help_text, parse_mode='HTML')

@group_router.message(F.text.regexp(r'^\/vv(?:\s+.+)?$'))
async def cmd_vv_in_group(message: Message, _: callable):
    """
    Handles the /vv command in group chats. Either synthesizes text directly,
    or extracts from a provided link, or uses the text from the replied-to message.
    """
    if message.reply_to_message:
        text_to_speak = message.reply_to_message.text or message.reply_to_message.caption or ""
        if not text_to_speak.strip():
            await message.reply(_("No text to synthesize."))
            logger.warning(f"Group {message.chat.id} sent /vv command but no text was found in the replied message.")
            return
    else:
        splitted = message.text.split(' ', maxsplit=1)
        if len(splitted) > 1:
            text_to_speak = splitted[1].strip()
        else:
            await message.reply(_("Please provide text after /vv or reply to a message."))
            logger.warning(f"Group {message.chat.id} sent /vv command without additional text or a reply.")
            return

    if text_to_speak.startswith("http://") or text_to_speak.startswith("https://"):
        try:
            text_page = await extract_text_from_url_static(text_to_speak)
            if len(text_page) < 200:
                text_page = await extract_text_from_url_dynamic(text_to_speak)
            if not text_page.strip():
                await message.reply(_("Could not extract text."))
                logger.warning(f"Group {message.chat.id} provided a URL but no text was extracted.")
                return

            await synthesize_text_to_audio_edge(
                text_page,
                str(message.from_user.id),
                message,
                logger,
                _
            )
            logger.info(f"Group {message.chat.id} provided URL to synthesize: {text_to_speak}")

        except Exception as e:
            await message.reply(_("Failed to process link: {error}").format(error=str(e)))
            logger.error(f"Failed to process URL from group {message.chat.id}: {text_to_speak} | Error: {e}")
    else:
        if not text_to_speak.strip():
            await message.reply(_("Empty text."))
            logger.warning(f"Group {message.chat.id} sent /vv command with empty text.")
            return

        await synthesize_text_to_audio_edge(
            text_to_speak,
            str(message.from_user.id),
            message,
            logger,
            _
        )
        logger.info(f"Group {message.chat.id} provided text to synthesize: {text_to_speak}")

